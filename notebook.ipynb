{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Completo: Treinamento e Avalia√ß√£o de Modelo de Detec√ß√£o\n",
    "\n",
    "Este notebook cont√©m o fluxo completo para treinar e avaliar um modelo de detec√ß√£o de objetos (Faster R-CNN). \n",
    "\n",
    "**Fluxo do Notebook:**\n",
    "1.  **Instala√ß√£o de Depend√™ncias:** Instala bibliotecas necess√°rias.\n",
    "2.  **Imports Globais:** Carrega todos os pacotes Python.\n",
    "3.  **Configura√ß√£o:** Define todos os par√¢metros, caminhos e hiperpar√¢metros.\n",
    "4.  **Fun√ß√µes Utilit√°rias:** Cont√©m a classe `Dataset`, as fun√ß√µes de transforma√ß√£o e a fun√ß√£o de cria√ß√£o do modelo.\n",
    "5.  **Engine de Treinamento:** Cont√©m as fun√ß√µes para treinar e validar uma √©poca.\n",
    "6.  **Script Principal de Treinamento:** Executa o fluxo de treinamento completo.\n",
    "7.  **Avalia√ß√£o Quantitativa (mAP):** Script para calcular as m√©tricas no modelo salvo.\n",
    "8.  **Teste Pr√°tico:** Script para visualizar a previs√£o do modelo em uma imagem de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T19:46:02.852961Z",
     "iopub.status.busy": "2025-06-19T19:46:02.852649Z",
     "iopub.status.idle": "2025-06-19T19:46:09.174328Z",
     "shell.execute_reply": "2025-06-19T19:46:09.173535Z",
     "shell.execute_reply.started": "2025-06-19T19:46:02.852939Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Instala√ß√£o de Depend√™ncias\n",
    "\n",
    "!pip install -q tqdm torchmetrics opencv-python\n",
    "!pip install -U albumentations\n",
    "\n",
    "print(\"Depend√™ncias prontas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T19:46:09.176406Z",
     "iopub.status.busy": "2025-06-19T19:46:09.176054Z",
     "iopub.status.idle": "2025-06-19T19:46:09.181654Z",
     "shell.execute_reply": "2025-06-19T19:46:09.180865Z",
     "shell.execute_reply.started": "2025-06-19T19:46:09.176378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Imports Globais\n",
    "\n",
    "# --- PyTorch e Torchvision ---\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- Manipula√ß√£o de Imagens e Dados ---\n",
    "from PIL import Image  \t# Biblioteca para abrir e manipular imagens (Pillow).\n",
    "import numpy as np\n",
    "import albumentations as A\t# Biblioteca para data augmentation.\n",
    "from pycocotools.coco import COCO\t# Utilit√°rio para manusear anota√ß√µes COCO.\n",
    "from albumentations.pytorch import ToTensorV2\t# Converte imagens (numpy/pil) para tensores PyTorch.\n",
    "\n",
    "# --- Utilit√°rios de Treinamento e Avalia√ß√£o ---\n",
    "from tqdm.auto import tqdm\t# Cria barras de progresso que funcionam bem em notebooks.\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision  # M√©trica padr√£o para detec√ß√£o de objetos.\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.transforms import functional as F # torchvision para usar a fun√ß√£o to_tensor.\n",
    "\n",
    "# --- Bibliotecas Padr√£o do Python ---\n",
    "import os \t\t# Para manipula√ß√£o de caminhos e arquivos.\n",
    "import glob  \t# Para encontrar arquivos\n",
    "import re  \t\t# Para usar express√µes regulares\n",
    "import random\t# Para opera√ß√µes de aleatoriedade\n",
    "import cv2  \t# OpenCV, para desenhar caixas e texto nas imagens de teste.\n",
    "import matplotlib.pyplot as plt # Para exibir imagens no notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T19:46:09.182678Z",
     "iopub.status.busy": "2025-06-19T19:46:09.182424Z",
     "iopub.status.idle": "2025-06-19T19:46:09.203747Z",
     "shell.execute_reply": "2025-06-19T19:46:09.203017Z",
     "shell.execute_reply.started": "2025-06-19T19:46:09.182652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# C√©lula 4: Configura√ß√£o\n",
    "\n",
    "# Define uma classe para centralizar todas as configura√ß√µes do projeto.\n",
    "class Config:\n",
    "    \n",
    "    # --- CAMINHO PARA O DATASET ---\n",
    "    DATA_ROOT_PATH = '/kaggle/input/coco-dataset/my_data'\n",
    "    \n",
    "    # --- CONFIGURA√á√ïES GERAIS ---\n",
    "    # Define o dispositivo de computa√ß√£o: 'cuda' (GPU) se dispon√≠vel, sen√£o 'cpu'.\n",
    "    DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    # Diret√≥rio onde os checkpoints do modelo ser√£o salvos durante o treinamento.\n",
    "    CHECKPOINT_DIR = '/kaggle/working/checkpoints/'\n",
    "    \n",
    "    # Define o n√∫mero total de classes. Como o modelo precisa de uma classe para o fundo (background),\n",
    "    # o n√∫mero √© (quantidade de classes reais + 1).\n",
    "    NUM_CLASSES = 4  # 3 classes (person, car, dog) + 1 background\n",
    "    \n",
    "    # Lista com os nomes das categorias que queremos extrair do dataset COCO.\n",
    "    CATEGORIAS_DESEJADAS = ['person', 'car', 'dog']\n",
    "    \n",
    "    # --- CONFIGURA√á√ïES DO DATASET ---\n",
    "    # Limita o n√∫mero de imagens para acelerar o treinamento e a valida√ß√£o.\n",
    "    IMAGE_LIMIT = 3000  \t# Limite para o conjunto de treino.\n",
    "    VAL_IMAGE_LIMIT = 300 \t# Limite para o conjunto de valida√ß√£o.\n",
    "\n",
    "    # Caminhos espec√≠ficos para os dados de treino e valida√ß√£o e seus arquivos de anota√ß√£o JSON.\n",
    "    TRAIN_DATA_DIR = os.path.join(DATA_ROOT_PATH, 'train/')\n",
    "    TRAIN_COCO = os.path.join(DATA_ROOT_PATH, 'annotations/instances_train2017.json')\n",
    "    VAL_DATA_DIR = os.path.join(DATA_ROOT_PATH, 'val/')\n",
    "    VAL_COCO = os.path.join(DATA_ROOT_PATH, 'annotations/instances_val2017.json')\n",
    "\n",
    "    # --- HIPERPAR√ÇMETROS DE TREINAMENTO ---\n",
    "    NUM_EPOCHS = 20  \t\t# N√∫mero total de √©pocas para treinar o modelo.\n",
    "    TRAIN_BATCH_SIZE = 1 \t# Quantidade de imagens por lote de treinamento.\n",
    "    TRAIN_SHUFFLE_DL = True\t# Embaralhar o dataset de treino a cada √©poca.\n",
    "    NUM_WORKERS_DL = 0  \t# N√∫mero de processos para carregar dados. 0 significa que ser√° na thread principal.\n",
    "\n",
    "    # Par√¢metros do otimizador SGD (Gradiente Descendente Estoc√°stico).\n",
    "    LR = 0.001  \t# Taxa de aprendizado (learning rate).\n",
    "    MOMENTUM = 0.9  # Momento, ajuda a acelerar o SGD na dire√ß√£o certa.\n",
    "    WEIGHT_DECAY = 0.0005  # Termo de regulariza√ß√£o L2 para evitar overfitting.\n",
    "\n",
    "# Cria uma inst√¢ncia da classe de configura√ß√£o para ser usada no restante do c√≥digo.\n",
    "config = Config()\n",
    "print(\"Configura√ß√µes definidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T19:46:09.205218Z",
     "iopub.status.busy": "2025-06-19T19:46:09.205019Z",
     "iopub.status.idle": "2025-06-19T19:46:09.224652Z",
     "shell.execute_reply": "2025-06-19T19:46:09.223933Z",
     "shell.execute_reply.started": "2025-06-19T19:46:09.205203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA 5: FUN√á√ïES UTILIT√ÅRIAS (VERS√ÉO FINAL E CORRIGIDA)\n",
    "# Esta vers√£o cont√©m a corre√ß√£o definitiva para o problema de tipo de dado.\n",
    "# ==============================================================================\n",
    "\n",
    "# Classe customizada de Dataset para o formato COCO, com filtros.\n",
    "class FilteredCOCODataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root, annotation, transforms=None, cats=None, limit=None):\n",
    "        \n",
    "        self.root = root  \t# Diret√≥rio das imagens.\n",
    "        self.transforms = transforms  # Transforma√ß√µes de data augmentation.\n",
    "        self.coco = COCO(annotation)  # Carrega o arquivo de anota√ß√µes COCO.\n",
    "        \n",
    "        # Pega os IDs num√©ricos das categorias desejadas (ex: 'person', 'car').\n",
    "        self.desired_cat_ids = set(self.coco.getCatIds(catNms=cats if cats else []))\n",
    "        \n",
    "        # Mapeia os IDs originais do COCO para os IDs do nosso modelo (1, 2, 3...).\n",
    "        self.coco_to_model_map = {coco_id: i + 1 for i, coco_id in enumerate(sorted(list(self.desired_cat_ids)))}\n",
    "        \n",
    "        # L√≥gica para limitar e balancear o dataset.\n",
    "        if limit and limit > 0 and len(self.desired_cat_ids) > 0:\n",
    "            \n",
    "            # Obt√©m uma lista de imagens para cada categoria desejada.\n",
    "            imgs_per_cat = {cat_id: self.coco.getImgIds(catIds=[cat_id]) for cat_id in self.desired_cat_ids}\n",
    "            \n",
    "            # Calcula quantas imagens pegar por categoria para atingir o limite.\n",
    "            limit_per_cat = int(limit / len(self.desired_cat_ids))\n",
    "            balanced_img_ids = set() \t# Usa um conjunto para evitar duplicatas.\n",
    "            \n",
    "            for cat_id in self.desired_cat_ids:\n",
    "                \n",
    "                image_list = imgs_per_cat[cat_id]\n",
    "                random.shuffle(image_list)  # Embaralha para pegar uma amostra aleat√≥ria.\n",
    "                balanced_img_ids.update(image_list[:limit_per_cat])\n",
    "                \n",
    "            final_ids = list(balanced_img_ids)\n",
    "            random.shuffle(final_ids) # Embaralha a lista final de IDs.\n",
    "            \n",
    "            # Garante que n√£o passamos do limite e ordena os IDs.\n",
    "            self.ids = sorted(final_ids[:limit])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Se n√£o houver limite, pega todas as imagens que cont√™m as categorias desejadas.\n",
    "            all_image_ids = set()\n",
    "            \n",
    "            for cat_id in self.desired_cat_ids:\n",
    "                all_image_ids.update(self.coco.getImgIds(catIds=[cat_id]))\n",
    "                \n",
    "            self.ids = list(sorted(list(all_image_ids)))\n",
    "\n",
    "    # M√©todo que carrega e retorna um √∫nico item (imagem e anota√ß√µes) do dataset.\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_id = self.ids[idx] # Pega o ID da imagem pelo √≠ndice.\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # Carrega informa√ß√µes da imagem e suas anota√ß√µes do COCO.\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "            coco_annotations = self.coco.loadAnns(ann_ids)\n",
    "            path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "            img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "            \n",
    "            boxes, labels = [], []\n",
    "            \n",
    "            for obj in coco_annotations:\n",
    "                \n",
    "                # Filtra apenas os objetos das categorias desejadas e com √°rea v√°lida.\n",
    "                if obj['category_id'] in self.desired_cat_ids and obj['bbox'][2] > 0 and obj['bbox'][3] > 0:\n",
    "                    \n",
    "                    boxes.append(obj['bbox']) # Bbox no formato [x, y, width, height].\n",
    "                    labels.append(self.coco_to_model_map[obj['category_id']]) # Usa o label mapeado.\n",
    "            \n",
    "            if not boxes: return None # Pula a imagem se n√£o tiver objetos de interesse.\n",
    "\n",
    "            # Converte as bboxes para o formato [xmin, ymin, xmax, ymax] exigido pelo PyTorch.\n",
    "            boxes_tensor = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
    "            boxes_tensor[:, 2:] += boxes_tensor[:, :2]\n",
    "            \n",
    "            # Cria o dicion√°rio 'target' com as anota√ß√µes formatadas.\n",
    "            target = {'boxes': boxes_tensor, 'labels': torch.as_tensor(labels, dtype=torch.int64)}\n",
    "\n",
    "            # Aplica as transforma√ß√µes (data augmentation).\n",
    "            if self.transforms:\n",
    "                \n",
    "                transformed = self.transforms(image=np.array(img), bboxes=target['boxes'].numpy(), labels=target['labels'].numpy())\n",
    "                img = transformed['image']\n",
    "                if len(transformed['bboxes']) == 0: return None \t# Pula se a augmenta√ß√£o removeu todas as bboxes.\n",
    "                target['boxes'] = torch.as_tensor(transformed['bboxes'], dtype=torch.float32).reshape(-1, 4)\n",
    "                target['labels'] = torch.as_tensor(transformed['labels'], dtype=torch.int64)\n",
    "            \n",
    "            # --- SOLU√á√ÉO DEFINITIVA ---\n",
    "            # Garante que a imagem seja um tensor de ponto flutuante (float) e normalizada para o intervalo [0, 1].\n",
    "            # ToTensorV2 deveria fazer isso, mas essa convers√£o manual garante a consist√™ncia.\n",
    "            if not img.is_floating_point():\n",
    "                img = img.to(torch.float32) / 255.0\n",
    "\n",
    "            return img, target\n",
    "        \n",
    "        except Exception:\n",
    "            \n",
    "            # Retorna None se houver qualquer erro ao processar a imagem (ex: arquivo corrompido).\n",
    "            return None\n",
    "\n",
    "    # M√©todo que retorna o n√∫mero total de amostras no dataset.\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.ids)\n",
    "\n",
    "# Fun√ß√£o que define a sequ√™ncia de transforma√ß√µes de imagem.\n",
    "def get_transform(train):\n",
    "    \n",
    "    transforms_list = []\n",
    "    bbox_params = None\n",
    "    \n",
    "    if train:\n",
    "        \n",
    "        # Se for para treinamento, aplica data augmentation.\n",
    "        transforms_list.append(A.HorizontalFlip(p=0.5)) # Espelhamento horizontal com 50% de chance.\n",
    "        transforms_list.append(A.RandomBrightnessContrast(p=0.2)) # Muda brilho e contraste.\n",
    "        \n",
    "        # Define os par√¢metros para as bounding boxes, para que elas se ajustem com as augmenta√ß√µes.\n",
    "        bbox_params = A.BboxParams(format='pascal_voc', label_fields=['labels'], min_area=1, min_visibility=0.1)\n",
    "    \n",
    "    # Sempre converte a imagem para um tensor PyTorch.\n",
    "    transforms_list.append(ToTensorV2())\n",
    "    \n",
    "    # Comp√µe (agrupa) as transforma√ß√µes em um √∫nico pipeline.\n",
    "    if bbox_params:\n",
    "        return A.Compose(transforms_list, bbox_params=bbox_params)\n",
    "    \n",
    "    else:\n",
    "        return A.Compose(transforms_list)\n",
    "\n",
    "# Fun√ß√£o de agrupamento para o DataLoader.\n",
    "def collate_fn(batch):\n",
    "    \n",
    "    # Filtra amostras que retornaram 'None' do __getitem__.\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    \n",
    "    if not batch:\n",
    "        return None, None\n",
    "    \n",
    "    # Separa as imagens e os alvos em duas tuplas separadas.\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Fun√ß√£o para criar o modelo de detec√ß√£o.\n",
    "def get_detection_model(num_classes):\n",
    "    \n",
    "    # Carrega um modelo Faster R-CNN com backbone ResNet50, pr√©-treinado no COCO.\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "    \n",
    "    # Obt√©m o n√∫mero de caracter√≠sticas de entrada do classificador.\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Substitui a \"cabe√ßa\" do classificador por uma nova, com o n√∫mero correto de classes para nosso problema.\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "print(\"C√©lula 5: Fun√ß√µes Utilit√°rias definidas (VERS√ÉO FINAL E CORRIGIDA).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T19:46:09.225558Z",
     "iopub.status.busy": "2025-06-19T19:46:09.22536Z",
     "iopub.status.idle": "2025-06-19T19:46:09.241712Z",
     "shell.execute_reply": "2025-06-19T19:46:09.241163Z",
     "shell.execute_reply.started": "2025-06-19T19:46:09.225544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA 6: ENGINE DE TREINAMENTO (VERS√ÉO CORRIGIDA)\n",
    "# ==============================================================================\n",
    "\n",
    "# Fun√ß√£o para treinar o modelo por uma √©poca.\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    \"\"\"\n",
    "    Executa uma √∫nica √©poca de treinamento. (Esta fun√ß√£o j√° estava correta)\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()  # Coloca o modelo em modo de treinamento.\n",
    "    prog_bar = tqdm(data_loader, total=len(data_loader), desc=f\"√âpoca {epoch+1} [Treino]\")\n",
    "    train_epoch_loss = 0\n",
    "    \n",
    "    # Itera sobre os lotes de dados do data_loader de treinamento.\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        \n",
    "        if data is None or data[0] is None:\n",
    "            continue  # Pula o lote se for inv√°lido.\n",
    "            \n",
    "        imgs, annotations = data\n",
    "        \n",
    "        # Move as imagens e anota√ß√µes para o dispositivo (GPU/CPU).\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        \n",
    "        # O modelo retorna um dicion√°rio de perdas (losses) quando est√° em modo de treino.\n",
    "        loss_dict = model(imgs, annotations)\n",
    "        \n",
    "        # Soma todas as perdas (ex: perda de classifica√ß√£o, perda de regress√£o da caixa).\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Verifica√ß√£o de seguran√ßa: se a perda for infinita ou NaN, pula a atualiza√ß√£o.\n",
    "        if not torch.isfinite(losses):\n",
    "            print(f\"ALERTA: Loss infinita na itera√ß√£o {i}, pulando batch.\")\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()   # Zera os gradientes acumulados.\n",
    "        losses.backward()  \t\t# Calcula os gradientes (backpropagation).\n",
    "        \n",
    "        # Limita a norma dos gradientes para evitar \"exploding gradients\".\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\t# Atualiza os pesos do modelo.\n",
    "        \n",
    "        train_epoch_loss += losses.item()  \t\t \t# Acumula a perda da √©poca.\n",
    "        prog_bar.set_postfix(loss=losses.item())\t# Atualiza a barra de progresso com a loss atual.\n",
    "\n",
    "    # Retorna a m√©dia da perda de treinamento da √©poca.\n",
    "    return train_epoch_loss / len(data_loader) if len(data_loader) > 0 else 0.0\n",
    "\n",
    "\n",
    "# --- FUN√á√ÉO 'evaluate' ---\n",
    "# Decorador que desativa o c√°lculo de gradientes, economizando mem√≥ria e acelerando a execu√ß√£o.\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Executa la evaluaci√≥n del modelo en el conjunto de datos de validaci√≥n.\n",
    "    \"\"\"\n",
    "    model.eval()\t# Coloca o modelo em modo de avalia√ß√£o.\n",
    "    \n",
    "    prog_bar = tqdm(data_loader, total=len(data_loader), desc=\"[Valida√ß√£o]\")\n",
    "    validation_loss = 0\n",
    "    \n",
    "    # Itera sobre os lotes de dados do data_loader de valida√ß√£o.\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        if data is None or data[0] is None:\n",
    "            continue\n",
    "\n",
    "        imgs, annotations = data\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        \n",
    "        was_training = model.training\t# Guarda o estado atual (que √© 'eval').\n",
    "        model.train()\t# Muda para modo 'train' para obter o dict de loss.\n",
    "        loss_dict = model(imgs, annotations)\n",
    "        model.train(was_training)\t\t# Restaura o estado original ('eval').\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        if torch.isfinite(losses): \n",
    "            validation_loss += losses.item()\n",
    "        \n",
    "        prog_bar.set_postfix(loss=losses.item())\n",
    "        \n",
    "    # Retorna a m√©dia da perda de valida√ß√£o.\n",
    "    return validation_loss / len(data_loader) if len(data_loader) > 0 else 0.0\n",
    "\n",
    "print(\"C√©lula 6: Fun√ß√µes de Engine definidas (com 'evaluate' corrigido).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# (As fun√ß√µes auxiliares como collate_fn, get_transform, etc., j√° foram definidas nas c√©lulas anteriores)\n",
    "\n",
    "# --- Fun√ß√£o para criar o modelo (vers√£o espec√≠fica para ResNet-50) ---\n",
    "def get_model_fasterrcnn_resnet50(num_classes):\n",
    "    \"\"\"\n",
    "    Carrega um modelo Faster R-CNN pr√©-treinado (backbone ResNet-50)\n",
    "    e o adapta para o n√∫mero de classes desejado.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Carrega o modelo com pesos pr√©-treinados.\n",
    "    model = fasterrcnn_resnet50_fpn(weights='FasterRCNN_ResNet50_FPN_Weights.DEFAULT')\n",
    "    \n",
    "    # Obt√©m o n√∫mero de features da camada de predi√ß√£o.\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Substitui a camada de predi√ß√£o por uma nova, adequada ao nosso n√∫mero de classes.\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "# --- 1. CONFIGURA√á√ÉO INICIAL ---\n",
    "print(\"--- SCRIPT DE TREINAMENTO: Faster R-CNN com ResNet-50 ---\")\n",
    "print(f\"Dispositivo selecionado: {config.DEVICE}\")\n",
    "\n",
    "# Define um diret√≥rio espec√≠fico para os checkpoints deste modelo.\n",
    "config.CHECKPOINT_DIR = './checkpoints/fasterrcnn_resnet50'\n",
    "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"Checkpoints ser√£o salvos em: {config.CHECKPOINT_DIR}\")\n",
    "\n",
    "# --- 2. DATASETS E DATALOADERS ---\n",
    "print(\"\\nConfigurando Datasets...\")\n",
    "# Cria o dataset de treinamento usando a classe customizada.\n",
    "dataset_train = FilteredCOCODataset(\n",
    "    root=config.TRAIN_DATA_DIR, \n",
    "    annotation=config.TRAIN_COCO, \n",
    "    transforms=get_transform(train=True), \n",
    "    cats=config.CATEGORIAS_DESEJADAS,\n",
    "    limit=config.IMAGE_LIMIT\n",
    ")\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=config.TRAIN_BATCH_SIZE, shuffle=config.TRAIN_SHUFFLE_DL, num_workers=config.NUM_WORKERS_DL, collate_fn=collate_fn)\n",
    "\n",
    "# Cria o dataset de valida√ß√£o.\n",
    "dataset_val = FilteredCOCODataset(\n",
    "    root=config.VAL_DATA_DIR, \n",
    "    annotation=config.VAL_COCO, \n",
    "    transforms=get_transform(train=False), \n",
    "    cats=config.CATEGORIAS_DESEJADAS,\n",
    "    limit=config.VAL_IMAGE_LIMIT\n",
    ")\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=config.NUM_WORKERS_DL, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Dataset de Treinamento: {len(dataset_train)} imagens.\")\n",
    "print(f\"Dataset de Valida√ß√£o: {len(dataset_val)} imagens.\")\n",
    "\n",
    "# --- 3. MODELO E OTIMIZADOR ---\n",
    "print(\"\\nCarregando modelo...\")\n",
    "model = get_model_fasterrcnn_resnet50(config.NUM_CLASSES)\n",
    "model.to(config.DEVICE) # Move o modelo para a GPU/CPU.\n",
    "\n",
    "# Seleciona os par√¢metros que precisam de gradientes para o otimizador.\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=config.LR, momentum=config.MOMENTUM, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "### MODIFICA√á√ÉO 1: L√ìGICA DE RETOMADA SIMPLIFICADA ###\n",
    "# --- 4. L√ìGICA DE CHECKPOINT (RETOMADA) ---\n",
    "print(\"\\nProcurando pelo melhor checkpoint para retomar...\")\n",
    "best_val_loss = float('inf') # Inicializa a melhor perda com infinito.\n",
    "start_epoch = 0 # √âpoca inicial √© 0.\n",
    "\n",
    "# Procura por um √∫nico arquivo de checkpoint, que representa o melhor modelo at√© agora.\n",
    "best_model_path = os.path.join(config.CHECKPOINT_DIR, \"best_model_checkpoint.pth\")\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    \n",
    "    print(f\"Retomando do melhor checkpoint encontrado: {best_model_path}\")\n",
    "    checkpoint = torch.load(best_model_path) \t# Carrega o checkpoint.\n",
    "    \n",
    "    # Restaura o estado do modelo, do otimizador e outros dados do treinamento.\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] \t# Define a √©poca de in√≠cio para a pr√≥xima.\n",
    "    best_val_loss = checkpoint.get('best_val_loss', float('inf')) \n",
    "    \n",
    "    print(f\"Retomado da √©poca {start_epoch}. Melhor loss de valida√ß√£o at√© agora: {best_val_loss:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Nenhum checkpoint encontrado. Iniciando do zero.\")\n",
    "\n",
    "\n",
    "### MODIFICA√á√ÉO 2: LOOP DE TREINAMENTO SIMPLIFICADO ###\n",
    "# --- 5. LOOP DE TREINAMENTO PRINCIPAL ---\n",
    "print(\"\\n--- Iniciando Treinamento ---\")\n",
    "for epoch in range(start_epoch, config.NUM_EPOCHS):\n",
    "    \n",
    "    # Executa uma √©poca de treino e uma de valida√ß√£o.\n",
    "    avg_train_loss = train_one_epoch(model, optimizer, data_loader_train, config.DEVICE, epoch)\n",
    "    avg_val_loss = evaluate(model, data_loader_val, config.DEVICE)\n",
    "    print(f\"--- Fim da √âpoca {epoch+1}/{config.NUM_EPOCHS} | Loss Treino: {avg_train_loss:.4f} | Loss Valida√ß√£o: {avg_val_loss:.4f} ---\")\n",
    "\n",
    "    # L√≥gica para salvar o melhor modelo.\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        \n",
    "        best_val_loss = avg_val_loss\n",
    "        \n",
    "        # O nome do arquivo √© fixo, ent√£o ele ser√° sempre sobrescrito com um modelo melhor.\n",
    "        best_checkpoint_path = os.path.join(config.CHECKPOINT_DIR, \"best_model_checkpoint.pth\")\n",
    "        \n",
    "        print(f\"üéâ Nova melhor loss de VALIDA√á√ÉO! Salvando checkpoint em: {best_checkpoint_path}\")\n",
    "\n",
    "        # Salva um dicion√°rio completo para permitir a retomada do treinamento.\n",
    "        checkpoint_data = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss\n",
    "        }\n",
    "        torch.save(checkpoint_data, best_checkpoint_path)\n",
    "\n",
    "    print(\"-\" * 50) # Separador visual entre as √©pocas.\n",
    "\n",
    "print(\"--- Treinamento Conclu√≠do ---\")\n",
    "print(f\"O melhor modelo (com loss de valida√ß√£o {best_val_loss:.4f}) est√° salvo em '{best_model_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T19:46:47.388824Z",
     "iopub.status.busy": "2025-06-19T19:46:47.388229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_mobilenet.py\n",
    "\n",
    "# --- Fun√ß√£o para criar o modelo ---\n",
    "def get_model_fasterrcnn_mobilenet(num_classes):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cria um modelo Faster R-CNN usando um backbone MobileNetV2.\n",
    "    Este √© um modelo mais leve que o ResNet-50.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Carrega o extrator de features (backbone) do MobileNetV2 pr√©-treinado no ImageNet.\n",
    "    backbone = torchvision.models.mobilenet_v2(weights='MobileNet_V2_Weights.DEFAULT').features\n",
    "    \n",
    "    # O n√∫mero de canais de sa√≠da do backbone √© necess√°rio para a pr√≥xima camada.\n",
    "    backbone.out_channels = 1280\n",
    "    \n",
    "    # Define o gerador de √¢ncoras para a Region Proposal Network (RPN).\n",
    "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "    \n",
    "    # Define a camada de RoI pooling.\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)\n",
    "    \n",
    "    # Constr√≥i o modelo Faster R-CNN com os componentes definidos.\n",
    "    model = FasterRCNN(backbone, num_classes=num_classes, rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler)\n",
    "    return model\n",
    "\n",
    "# --- 1. CONFIGURA√á√ÉO INICIAL ---\n",
    "print(\"--- SCRIPT DE TREINAMENTO: Faster R-CNN com MobileNetV2 ---\")\n",
    "print(f\"Dispositivo selecionado: {config.DEVICE}\")\n",
    "\n",
    "# Diret√≥rio de checkpoint espec√≠fico para este modelo.\n",
    "config.CHECKPOINT_DIR = './checkpoints/fasterrcnn_mobilenetv2'\n",
    "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoints ser√£o salvos em: {config.CHECKPOINT_DIR}\")\n",
    "\n",
    "# --- 2. DATASETS E DATALOADERS ---\n",
    "# (Esta se√ß√£o √© id√™ntica √† do script ResNet-50)\n",
    "print(\"\\nConfigurando Datasets...\")\n",
    "\n",
    "dataset_train = FilteredCOCODataset(root=config.TRAIN_DATA_DIR, annotation=config.TRAIN_COCO, transforms=get_transform(train=True), cats=config.CATEGORIAS_DESEJADAS, limit=config.IMAGE_LIMIT)\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=config.TRAIN_BATCH_SIZE, shuffle=config.TRAIN_SHUFFLE_DL, num_workers=config.NUM_WORKERS_DL, collate_fn=collate_fn)\n",
    "\n",
    "dataset_val = FilteredCOCODataset(root=config.VAL_DATA_DIR, annotation=config.VAL_COCO, transforms=get_transform(train=False), cats=config.CATEGORIAS_DESEJADAS, limit=config.VAL_IMAGE_LIMIT)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=config.NUM_WORKERS_DL, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Dataset de Treinamento: {len(dataset_train)} imagens.\")\n",
    "print(f\"Dataset de Valida√ß√£o: {len(dataset_val)} imagens.\")\n",
    "\n",
    "# --- 3. MODELO E OTIMIZADOR ---\n",
    "print(\"\\nCarregando modelo...\")\n",
    "\n",
    "model = get_model_fasterrcnn_mobilenet(config.NUM_CLASSES)\n",
    "model.to(config.DEVICE)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=config.LR, momentum=config.MOMENTUM, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "# --- 4. L√ìGICA DE CHECKPOINT (RETOMADA) ---\n",
    "best_val_loss = float('inf')\n",
    "start_epoch = 0\n",
    "# Procura por todos os checkpoints salvos anteriormente.\n",
    "checkpoint_files = glob.glob(os.path.join(config.CHECKPOINT_DIR, 'checkpoint_epoch_*.pth'))\n",
    "\n",
    "if checkpoint_files:\n",
    "    \n",
    "    # Extrai o n√∫mero da √©poca do nome de cada arquivo.\n",
    "    epochs = [int(re.search(r'(\\d+)', os.path.basename(f)).group(1)) for f in checkpoint_files]\n",
    "    latest_epoch = max(epochs) # Encontra a √©poca mais recente.\n",
    "    latest_checkpoint_path = os.path.join(config.CHECKPOINT_DIR, f'checkpoint_epoch_{latest_epoch}.pth')\n",
    "    \n",
    "    print(f\"\\nRetomando do checkpoint: {latest_checkpoint_path}\")\n",
    "    \n",
    "    checkpoint = torch.load(latest_checkpoint_path)\n",
    "    \n",
    "    # Restaura o estado do modelo, otimizador e outros dados.\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_val_loss = checkpoint.get('best_val_loss', float('inf')) \n",
    "    \n",
    "    print(f\"Retomado da √©poca {start_epoch}. Melhor loss de valida√ß√£o: {best_val_loss:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNenhum checkpoint encontrado. Iniciando do zero.\")\n",
    "\n",
    "# --- 5. LOOP DE TREINAMENTO PRINCIPAL ---\n",
    "print(\"\\n--- Iniciando Treinamento ---\")\n",
    "for epoch in range(start_epoch, config.NUM_EPOCHS):\n",
    "    \n",
    "    avg_train_loss = train_one_epoch(model, optimizer, data_loader_train, config.DEVICE, epoch)\n",
    "    avg_val_loss = evaluate(model, data_loader_val, config.DEVICE)\n",
    "    \n",
    "    print(f\"--- Fim da √âpoca {epoch+1}/{config.NUM_EPOCHS} | Loss Treino: {avg_train_loss:.4f} | Loss Valida√ß√£o: {avg_val_loss:.4f} ---\")\n",
    "\n",
    "    # Se a loss de valida√ß√£o atual for a melhor at√© agora...\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        \n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_path = os.path.join(config.CHECKPOINT_DIR, \"best_model.pth\")\n",
    "        # ...salva apenas os pesos do modelo (state_dict) no arquivo 'best_model.pth'.\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        \n",
    "        print(f\"üéâ Nova melhor loss de VALIDA√á√ÉO! Modelo salvo em: {best_model_path}\")\n",
    "\n",
    "    # Salva um checkpoint completo ao final de CADA √©poca.\n",
    "    checkpoint_path = os.path.join(config.CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "    checkpoint = {'epoch': epoch + 1, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'train_loss': avg_train_loss, 'val_loss': avg_val_loss, 'best_val_loss': best_val_loss}\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    print(f\"Checkpoint da √©poca {epoch+1} salvo.\\n\")\n",
    "\n",
    "print(\"--- Treinamento Conclu√≠do ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-19T00:47:19.38Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_efficientnet.py (Vers√£o que salva apenas o melhor modelo)\n",
    "\n",
    "# --- Fun√ß√£o para criar o modelo ---\n",
    "def get_model_fasterrcnn_efficientnet(num_classes):\n",
    "    \"\"\"\n",
    "    Cria um modelo Faster R-CNN usando um backbone EfficientNet-B0.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Carrega o extrator de features (backbone) do EfficientNet-B0 pr√©-treinado.\n",
    "    backbone = torchvision.models.efficientnet_b0(weights='EfficientNet_B0_Weights.DEFAULT').features\n",
    "    \n",
    "    # Define manualmente o n√∫mero de canais de sa√≠da do backbone.\n",
    "    backbone.out_channels = 1280\n",
    "    \n",
    "    # Define o gerador de √¢ncoras e o RoI Pooler, similar ao MobileNet.\n",
    "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)\n",
    "    \n",
    "    # Constr√≥i o modelo Faster R-CNN final.\n",
    "    model = FasterRCNN(backbone, num_classes=num_classes, rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# --- 1. CONFIGURA√á√ÉO INICIAL ---\n",
    "print(\"--- SCRIPT DE TREINAMENTO: Faster R-CNN com EfficientNet-B0 ---\")\n",
    "print(f\"Dispositivo selecionado: {config.DEVICE}\")\n",
    "\n",
    "# Diret√≥rio de checkpoint espec√≠fico para este modelo.\n",
    "config.CHECKPOINT_DIR = './checkpoints/fasterrcnn_efficientnetb0'\n",
    "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoints ser√£o salvos em: {config.CHECKPOINT_DIR}\")\n",
    "\n",
    "# --- 2. DATASETS E DATALOADERS ---\n",
    "# (Esta se√ß√£o √© id√™ntica √† dos outros scripts)\n",
    "print(\"\\nConfigurando Datasets...\")\n",
    "dataset_train = FilteredCOCODataset(root=config.TRAIN_DATA_DIR, annotation=config.TRAIN_COCO, transforms=get_transform(train=True), cats=config.CATEGORIAS_DESEJADAS, limit=config.IMAGE_LIMIT)\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=config.TRAIN_BATCH_SIZE, shuffle=config.TRAIN_SHUFFLE_DL, num_workers=config.NUM_WORKERS_DL, collate_fn=collate_fn)\n",
    "\n",
    "dataset_val = FilteredCOCODataset(root=config.VAL_DATA_DIR, annotation=config.VAL_COCO, transforms=get_transform(train=False), cats=config.CATEGORIAS_DESEJADAS, limit=config.VAL_IMAGE_LIMIT)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=config.NUM_WORKERS_DL, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Dataset de Treinamento: {len(dataset_train)} imagens.\")\n",
    "print(f\"Dataset de Valida√ß√£o: {len(dataset_val)} imagens.\")\n",
    "\n",
    "# --- 3. MODELO E OTIMIZADOR ---\n",
    "print(\"\\nCarregando modelo...\")\n",
    "model = get_model_fasterrcnn_efficientnet(config.NUM_CLASSES)\n",
    "model.to(config.DEVICE)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=config.LR, momentum=config.MOMENTUM, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "### MODIFICA√á√ÉO 1: L√ìGICA DE RETOMADA SIMPLIFICADA ###\n",
    "# --- 4. L√ìGICA DE CHECKPOINT (RETOMADA) ---\n",
    "# (Esta se√ß√£o √© id√™ntica √† do script ResNet-50)\n",
    "print(\"\\nProcurando pelo melhor checkpoint para retomar...\")\n",
    "best_val_loss = float('inf')\n",
    "start_epoch = 0\n",
    "\n",
    "# Procura pelo arquivo √∫nico do melhor modelo.\n",
    "best_model_path = os.path.join(config.CHECKPOINT_DIR, \"best_model_checkpoint.pth\")\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    \n",
    "    print(f\"Retomando do melhor checkpoint encontrado: {best_model_path}\")\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    \n",
    "    # Restaura o estado completo do treinamento.\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_val_loss = checkpoint.get('best_val_loss', float('inf')) \n",
    "    \n",
    "    print(f\"Retomado da √©poca {start_epoch}. Melhor loss de valida√ß√£o at√© agora: {best_val_loss:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Nenhum checkpoint encontrado. Iniciando do zero.\")\n",
    "\n",
    "\n",
    "### MODIFICA√á√ÉO 2: LOOP DE TREINAMENTO SIMPLIFICADO ###\n",
    "# --- 5. LOOP DE TREINAMENTO PRINCIPAL ---\n",
    "# (Esta se√ß√£o √© id√™ntica √† do script ResNet-50)\n",
    "print(\"\\n--- Iniciando Treinamento ---\")\n",
    "for epoch in range(start_epoch, config.NUM_EPOCHS):\n",
    "    \n",
    "    avg_train_loss = train_one_epoch(model, optimizer, data_loader_train, config.DEVICE, epoch)\n",
    "    avg_val_loss = evaluate(model, data_loader_val, config.DEVICE)\n",
    "    print(f\"--- Fim da √âpoca {epoch+1}/{config.NUM_EPOCHS} | Loss Treino: {avg_train_loss:.4f} | Loss Valida√ß√£o: {avg_val_loss:.4f} ---\")\n",
    "\n",
    "    # Se a loss de valida√ß√£o for a melhor, salva um checkpoint completo e sobrescreve o anterior.\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        \n",
    "        best_val_loss = avg_val_loss\n",
    "        best_checkpoint_path = os.path.join(config.CHECKPOINT_DIR, \"best_model_checkpoint.pth\")\n",
    "        print(f\"üéâ Nova melhor loss de VALIDA√á√ÉO! Salvando checkpoint em: {best_checkpoint_path}\")\n",
    "\n",
    "        checkpoint_data = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss\n",
    "        }\n",
    "        torch.save(checkpoint_data, best_checkpoint_path)\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"--- Treinamento Conclu√≠do ---\")\n",
    "print(f\"O melhor modelo (com loss de valida√ß√£o {best_val_loss:.4f}) est√° salvo em '{best_model_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# C√©lula 10: Avalia√ß√£o Quantitativa (mAP)\n",
    "\n",
    "# Fun√ß√£o para rodar a avalia√ß√£o de mAP (Mean Average Precision).\n",
    "def run_map_evaluation(weights_path):\n",
    "    \n",
    "    print(\"\\n--- Iniciando Avalia√ß√£o de M√©tricas mAP ---\")\n",
    "    \n",
    "    # Verifica se o data_loader de valida√ß√£o existe.\n",
    "    if 'data_loader_val' not in locals():\n",
    "        \n",
    "        print(\"DataLoader de valida√ß√£o n√£o encontrado. Execute a c√©lula de treinamento primeiro.\")\n",
    "        return\n",
    "\n",
    "    # Cria uma nova inst√¢ncia do modelo para avalia√ß√£o.\n",
    "    eval_model = get_detection_model(config.NUM_CLASSES)\n",
    "    \n",
    "    # Carrega os pesos do melhor modelo treinado.\n",
    "    eval_model.load_state_dict(torch.load(weights_path, map_location=config.DEVICE))\n",
    "    eval_model.to(config.DEVICE)\n",
    "    eval_model.eval() # Coloca o modelo em modo de avalia√ß√£o.\n",
    "    \n",
    "    # Inicializa o objeto de m√©trica da torchmetrics para calcular o mAP.\n",
    "    # box_format='xyxy' corresponde ao formato [xmin, ymin, xmax, ymax].\n",
    "    metric = MeanAveragePrecision(box_format='xyxy', iou_type=\"bbox\")\n",
    "    prog_bar = tqdm(data_loader_val, total=len(data_loader_val), desc=\"Calculando M√©tricas mAP...\")\n",
    "    \n",
    "    # Itera sobre o dataset de valida√ß√£o.\n",
    "    for images, targets in prog_bar:\n",
    "        \n",
    "        images = list(img.to(config.DEVICE) for img in images)\n",
    "        \n",
    "        # O modelo em modo 'eval' retorna uma lista de predi√ß√µes.\n",
    "        predictions = eval_model(images)\n",
    "        \n",
    "        # Formata os alvos (ground truth) para o formato esperado pela m√©trica.\n",
    "        formatted_targets = [{\"boxes\": t[\"boxes\"], \"labels\": t[\"labels\"]} for t in targets]\n",
    "        \n",
    "        # Atualiza a m√©trica com as predi√ß√µes e os alvos do lote atual.\n",
    "        metric.update(predictions, formatted_targets)\n",
    "        \n",
    "    # Calcula os resultados finais da m√©trica acumulada.\n",
    "    results = metric.compute()\n",
    "    print(\"\\n--- Resultados da Avalia√ß√£o (mAP para Bounding Box) ---\")\n",
    "    \n",
    "    # Imprime os resultados de forma organizada.\n",
    "    for key, value in results.items():\n",
    "        \n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"{key:<25}: {value.item():.4f}\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "\n",
    "# Executa a avalia√ß√£o no melhor modelo salvo pelo script de treinamento.\n",
    "# Nota: Este caminho pode precisar de ajuste dependendo de qual script de treino foi executado.\n",
    "best_model_file = os.path.join(config.CHECKPOINT_DIR, 'best_model.pth')\n",
    "if os.path.exists(best_model_file):\n",
    "    run_map_evaluation(best_model_file)\n",
    "    \n",
    "else:\n",
    "    print(f\"Arquivo '{best_model_file}' n√£o encontrado. Execute o treinamento primeiro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# C√©lula 11: Teste Pr√°tico em uma Imagem\n",
    "\n",
    "# Fun√ß√£o para testar o modelo em uma √∫nica imagem e visualizar as detec√ß√µes.\n",
    "def test_single_image(weights_path, image_path, threshold=0.5):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"ERRO: Arquivo de imagem n√£o encontrado em: {image_path}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Iniciando Teste Pr√°tico ---\")\n",
    "    \n",
    "    # Cria uma inst√¢ncia do modelo e carrega os pesos treinados.\n",
    "    test_model = get_detection_model(config.NUM_CLASSES)\n",
    "    test_model.load_state_dict(torch.load(weights_path, map_location=config.DEVICE))\n",
    "    test_model.to(config.DEVICE)\n",
    "    test_model.eval() # Modo de avalia√ß√£o.\n",
    "\n",
    "    # Carrega a imagem de teste usando PIL e converte para RGB.\n",
    "    image_pil = Image.open(image_path).convert('RGB')\n",
    "    # Converte a imagem PIL para um tensor PyTorch.\n",
    "    image_tensor = F.to_tensor(image_pil)\n",
    "    \n",
    "    # Realiza a predi√ß√£o sem calcular gradientes.\n",
    "    with torch.no_grad():\n",
    "        # O modelo espera uma lista de imagens, por isso [image_tensor].\n",
    "        prediction = test_model([image_tensor.to(config.DEVICE)])[0]\n",
    "\n",
    "    # Converte a imagem PIL para o formato OpenCV (BGR) para desenhar nela.\n",
    "    image_cv = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Define os nomes das classes para exibi√ß√£o.\n",
    "    CLASS_NAMES = ['background'] + config.CATEGORIAS_DESEJADAS\n",
    "    # Gera cores aleat√≥rias para cada classe.\n",
    "    colors = [[random.randint(0, 255) for _ in range(3)] for _ in CLASS_NAMES]\n",
    "\n",
    "    # Filtra as predi√ß√µes com base no limiar de confian√ßa (score).\n",
    "    scores = prediction['scores'].cpu().numpy()\n",
    "    high_confidence_indices = scores > threshold\n",
    "    boxes = prediction['boxes'][high_confidence_indices].cpu().numpy().astype(int)\n",
    "    labels = prediction['labels'][high_confidence_indices].cpu().numpy()\n",
    "\n",
    "    print(f\"Encontrados {len(boxes)} objetos com confian√ßa > {threshold}\")\n",
    "\n",
    "    # Itera sobre os objetos detectados para desenhar na imagem.\n",
    "    for i in range(len(boxes)):\n",
    "        box = boxes[i]\n",
    "        label_id = labels[i]\n",
    "        class_name = CLASS_NAMES[label_id]\n",
    "        color = colors[label_id]\n",
    "        score = scores[i]\n",
    "        text = f\"{class_name}: {score:.2f}\" # Texto com nome da classe e score.\n",
    "        \n",
    "        # Desenha o ret√¢ngulo (bounding box).\n",
    "        cv2.rectangle(image_cv, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
    "        # Calcula o tamanho do texto para criar um fundo.\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
    "        # Desenha um ret√¢ngulo de fundo para o texto.\n",
    "        cv2.rectangle(image_cv, (box[0], box[1] - text_height - 5), (box[0] + text_width, box[1] - 5), color, -1)\n",
    "        # Escreve o texto.\n",
    "        cv2.putText(image_cv, text, (box[0], box[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1)\n",
    "\n",
    "    # Exibe a imagem final com as detec√ß√µes usando Matplotlib.\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Converte a imagem de volta para RGB para exibi√ß√£o correta no Matplotlib.\n",
    "    plt.imshow(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off') # Remove os eixos.\n",
    "    plt.show()\n",
    "\n",
    "# --- PARA USAR ESTA C√âLULA ---\n",
    "# 1. Fa√ßa upload de uma imagem de teste para o seu notebook.\n",
    "# 2. Atualize os caminhos abaixo.\n",
    "# Caminho para o melhor modelo salvo. Verifique se o nome/caminho est√° correto.\n",
    "best_model_file = os.path.join(config.CHECKPOINT_DIR, 'best_model.pth')\n",
    "# Caminho para a sua imagem de teste.\n",
    "test_image_path = \"/kaggle/input/coco-dataset/my_data/val/000000000785.jpg\" # <--- MUDE AQUI PARA SUA IMAGEM\n",
    "\n",
    "# Verifica se os arquivos existem antes de tentar rodar o teste.\n",
    "if os.path.exists(best_model_file) and os.path.exists(test_image_path):\n",
    "    test_single_image(best_model_file, test_image_path, threshold=0.5)\n",
    "else:\n",
    "    print(\"Arquivo 'best_model.pth' ou imagem de teste n√£o encontrados. Execute o treinamento primeiro e verifique o caminho da imagem.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7684498,
     "sourceId": 12199220,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
