{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Completo: Treinamento e Avaliação de Modelo de Detecção\n",
    "\n",
    "Este notebook contém o fluxo completo para treinar e avaliar um modelo de detecção de objetos (Faster R-CNN). \n",
    "\n",
    "**Fluxo do Notebook:**\n",
    "1.  **Instalação de Dependências:** Instala bibliotecas necessárias.\n",
    "2.  **Imports Globais:** Carrega todos os pacotes Python.\n",
    "3.  **Configuração:** Define todos os parâmetros, caminhos e hiperparâmetros.\n",
    "4.  **Funções Utilitárias:** Contém a classe `Dataset`, as funções de transformação e a função de criação do modelo.\n",
    "5.  **Engine de Treinamento:** Contém as funções para treinar e validar uma época.\n",
    "6.  **Script Principal de Treinamento:** Executa o fluxo de treinamento completo.\n",
    "7.  **Avaliação Quantitativa (mAP):** Script para calcular as métricas no modelo salvo.\n",
    "8.  **Teste Prático:** Script para visualizar a previsão do modelo em uma imagem de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T19:46:02.852961Z",
     "iopub.status.busy": "2025-06-19T19:46:02.852649Z",
     "iopub.status.idle": "2025-06-19T19:46:09.174328Z",
     "shell.execute_reply": "2025-06-19T19:46:09.173535Z",
     "shell.execute_reply.started": "2025-06-19T19:46:02.852939Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Instalação de Dependências\n",
    "\n",
    "!pip install -q tqdm torchmetrics opencv-python\n",
    "!pip install -U albumentations\n",
    "\n",
    "print(\"Dependências prontas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T19:46:09.176406Z",
     "iopub.status.busy": "2025-06-19T19:46:09.176054Z",
     "iopub.status.idle": "2025-06-19T19:46:09.181654Z",
     "shell.execute_reply": "2025-06-19T19:46:09.180865Z",
     "shell.execute_reply.started": "2025-06-19T19:46:09.176378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Imports Globais\n",
    "\n",
    "# --- PyTorch e Torchvision ---\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- Manipulação de Imagens e Dados ---\n",
    "from PIL import Image  \t# Biblioteca para abrir e manipular imagens (Pillow).\n",
    "import numpy as np\n",
    "import albumentations as A\t# Biblioteca para data augmentation.\n",
    "from pycocotools.coco import COCO\t# Utilitário para manusear anotações COCO.\n",
    "from albumentations.pytorch import ToTensorV2\t# Converte imagens (numpy/pil) para tensores PyTorch.\n",
    "\n",
    "# --- Utilitários de Treinamento e Avaliação ---\n",
    "from tqdm.auto import tqdm\t# Cria barras de progresso que funcionam bem em notebooks.\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision  # Métrica padrão para detecção de objetos.\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.transforms import functional as F # torchvision para usar a função to_tensor.\n",
    "\n",
    "# --- Bibliotecas Padrão do Python ---\n",
    "import os \t\t# Para manipulação de caminhos e arquivos.\n",
    "import glob  \t# Para encontrar arquivos\n",
    "import re  \t\t# Para usar expressões regulares\n",
    "import random\t# Para operações de aleatoriedade\n",
    "import cv2  \t# OpenCV, para desenhar caixas e texto nas imagens de teste.\n",
    "import matplotlib.pyplot as plt # Para exibir imagens no notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T19:46:09.182678Z",
     "iopub.status.busy": "2025-06-19T19:46:09.182424Z",
     "iopub.status.idle": "2025-06-19T19:46:09.203747Z",
     "shell.execute_reply": "2025-06-19T19:46:09.203017Z",
     "shell.execute_reply.started": "2025-06-19T19:46:09.182652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Célula 4: Configuração\n",
    "\n",
    "# Define uma classe para centralizar todas as configurações do projeto.\n",
    "class Config:\n",
    "    \n",
    "    # --- CAMINHO PARA O DATASET ---\n",
    "    DATA_ROOT_PATH = '/kaggle/input/coco-dataset/my_data'\n",
    "    \n",
    "    # --- CONFIGURAÇÕES GERAIS ---\n",
    "    # Define o dispositivo de computação: 'cuda' (GPU) se disponível, senão 'cpu'.\n",
    "    DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    # Diretório onde os checkpoints do modelo serão salvos durante o treinamento.\n",
    "    CHECKPOINT_DIR = '/kaggle/working/checkpoints/'\n",
    "    \n",
    "    # Define o número total de classes. Como o modelo precisa de uma classe para o fundo (background),\n",
    "    # o número é (quantidade de classes reais + 1).\n",
    "    NUM_CLASSES = 4  # 3 classes (person, car, dog) + 1 background\n",
    "    \n",
    "    # Lista com os nomes das categorias que queremos extrair do dataset COCO.\n",
    "    CATEGORIAS_DESEJADAS = ['person', 'car', 'dog']\n",
    "    \n",
    "    # --- CONFIGURAÇÕES DO DATASET ---\n",
    "    # Limita o número de imagens para acelerar o treinamento e a validação.\n",
    "    IMAGE_LIMIT = 3000  \t# Limite para o conjunto de treino.\n",
    "    VAL_IMAGE_LIMIT = 300 \t# Limite para o conjunto de validação.\n",
    "\n",
    "    # Caminhos específicos para os dados de treino e validação e seus arquivos de anotação JSON.\n",
    "    TRAIN_DATA_DIR = os.path.join(DATA_ROOT_PATH, 'train/')\n",
    "    TRAIN_COCO = os.path.join(DATA_ROOT_PATH, 'annotations/instances_train2017.json')\n",
    "    VAL_DATA_DIR = os.path.join(DATA_ROOT_PATH, 'val/')\n",
    "    VAL_COCO = os.path.join(DATA_ROOT_PATH, 'annotations/instances_val2017.json')\n",
    "\n",
    "    # --- HIPERPARÂMETROS DE TREINAMENTO ---\n",
    "    NUM_EPOCHS = 20  \t\t# Número total de épocas para treinar o modelo.\n",
    "    TRAIN_BATCH_SIZE = 1 \t# Quantidade de imagens por lote de treinamento.\n",
    "    TRAIN_SHUFFLE_DL = True\t# Embaralhar o dataset de treino a cada época.\n",
    "    NUM_WORKERS_DL = 0  \t# Número de processos para carregar dados. 0 significa que será na thread principal.\n",
    "\n",
    "    # Parâmetros do otimizador SGD (Gradiente Descendente Estocástico).\n",
    "    LR = 0.001  \t# Taxa de aprendizado (learning rate).\n",
    "    MOMENTUM = 0.9  # Momento, ajuda a acelerar o SGD na direção certa.\n",
    "    WEIGHT_DECAY = 0.0005  # Termo de regularização L2 para evitar overfitting.\n",
    "\n",
    "# Cria uma instância da classe de configuração para ser usada no restante do código.\n",
    "config = Config()\n",
    "print(\"Configurações definidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T19:46:09.205218Z",
     "iopub.status.busy": "2025-06-19T19:46:09.205019Z",
     "iopub.status.idle": "2025-06-19T19:46:09.224652Z",
     "shell.execute_reply": "2025-06-19T19:46:09.223933Z",
     "shell.execute_reply.started": "2025-06-19T19:46:09.205203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 5: FUNÇÕES UTILITÁRIAS (VERSÃO FINAL E CORRIGIDA)\n",
    "# Esta versão contém a correção definitiva para o problema de tipo de dado.\n",
    "# ==============================================================================\n",
    "\n",
    "# Classe customizada de Dataset para o formato COCO, com filtros.\n",
    "class FilteredCOCODataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root, annotation, transforms=None, cats=None, limit=None):\n",
    "        \n",
    "        self.root = root  \t# Diretório das imagens.\n",
    "        self.transforms = transforms  # Transformações de data augmentation.\n",
    "        self.coco = COCO(annotation)  # Carrega o arquivo de anotações COCO.\n",
    "        \n",
    "        # Pega os IDs numéricos das categorias desejadas (ex: 'person', 'car').\n",
    "        self.desired_cat_ids = set(self.coco.getCatIds(catNms=cats if cats else []))\n",
    "        \n",
    "        # Mapeia os IDs originais do COCO para os IDs do nosso modelo (1, 2, 3...).\n",
    "        self.coco_to_model_map = {coco_id: i + 1 for i, coco_id in enumerate(sorted(list(self.desired_cat_ids)))}\n",
    "        \n",
    "        # Lógica para limitar e balancear o dataset.\n",
    "        if limit and limit > 0 and len(self.desired_cat_ids) > 0:\n",
    "            \n",
    "            # Obtém uma lista de imagens para cada categoria desejada.\n",
    "            imgs_per_cat = {cat_id: self.coco.getImgIds(catIds=[cat_id]) for cat_id in self.desired_cat_ids}\n",
    "            \n",
    "            # Calcula quantas imagens pegar por categoria para atingir o limite.\n",
    "            limit_per_cat = int(limit / len(self.desired_cat_ids))\n",
    "            balanced_img_ids = set() \t# Usa um conjunto para evitar duplicatas.\n",
    "            \n",
    "            for cat_id in self.desired_cat_ids:\n",
    "                \n",
    "                image_list = imgs_per_cat[cat_id]\n",
    "                random.shuffle(image_list)  # Embaralha para pegar uma amostra aleatória.\n",
    "                balanced_img_ids.update(image_list[:limit_per_cat])\n",
    "                \n",
    "            final_ids = list(balanced_img_ids)\n",
    "            random.shuffle(final_ids) # Embaralha a lista final de IDs.\n",
    "            \n",
    "            # Garante que não passamos do limite e ordena os IDs.\n",
    "            self.ids = sorted(final_ids[:limit])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Se não houver limite, pega todas as imagens que contêm as categorias desejadas.\n",
    "            all_image_ids = set()\n",
    "            \n",
    "            for cat_id in self.desired_cat_ids:\n",
    "                all_image_ids.update(self.coco.getImgIds(catIds=[cat_id]))\n",
    "                \n",
    "            self.ids = list(sorted(list(all_image_ids)))\n",
    "\n",
    "    # Método que carrega e retorna um único item (imagem e anotações) do dataset.\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_id = self.ids[idx] # Pega o ID da imagem pelo índice.\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # Carrega informações da imagem e suas anotações do COCO.\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "            coco_annotations = self.coco.loadAnns(ann_ids)\n",
    "            path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "            img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "            \n",
    "            boxes, labels = [], []\n",
    "            \n",
    "            for obj in coco_annotations:\n",
    "                \n",
    "                # Filtra apenas os objetos das categorias desejadas e com área válida.\n",
    "                if obj['category_id'] in self.desired_cat_ids and obj['bbox'][2] > 0 and obj['bbox'][3] > 0:\n",
    "                    \n",
    "                    boxes.append(obj['bbox']) # Bbox no formato [x, y, width, height].\n",
    "                    labels.append(self.coco_to_model_map[obj['category_id']]) # Usa o label mapeado.\n",
    "            \n",
    "            if not boxes: return None # Pula a imagem se não tiver objetos de interesse.\n",
    "\n",
    "            # Converte as bboxes para o formato [xmin, ymin, xmax, ymax] exigido pelo PyTorch.\n",
    "            boxes_tensor = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
    "            boxes_tensor[:, 2:] += boxes_tensor[:, :2]\n",
    "            \n",
    "            # Cria o dicionário 'target' com as anotações formatadas.\n",
    "            target = {'boxes': boxes_tensor, 'labels': torch.as_tensor(labels, dtype=torch.int64)}\n",
    "\n",
    "            # Aplica as transformações (data augmentation).\n",
    "            if self.transforms:\n",
    "                \n",
    "                transformed = self.transforms(image=np.array(img), bboxes=target['boxes'].numpy(), labels=target['labels'].numpy())\n",
    "                img = transformed['image']\n",
    "                if len(transformed['bboxes']) == 0: return None \t# Pula se a augmentação removeu todas as bboxes.\n",
    "                target['boxes'] = torch.as_tensor(transformed['bboxes'], dtype=torch.float32).reshape(-1, 4)\n",
    "                target['labels'] = torch.as_tensor(transformed['labels'], dtype=torch.int64)\n",
    "            \n",
    "            # --- SOLUÇÃO DEFINITIVA ---\n",
    "            # Garante que a imagem seja um tensor de ponto flutuante (float) e normalizada para o intervalo [0, 1].\n",
    "            # ToTensorV2 deveria fazer isso, mas essa conversão manual garante a consistência.\n",
    "            if not img.is_floating_point():\n",
    "                img = img.to(torch.float32) / 255.0\n",
    "\n",
    "            return img, target\n",
    "        \n",
    "        except Exception:\n",
    "            \n",
    "            # Retorna None se houver qualquer erro ao processar a imagem (ex: arquivo corrompido).\n",
    "            return None\n",
    "\n",
    "    # Método que retorna o número total de amostras no dataset.\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.ids)\n",
    "\n",
    "# Função que define a sequência de transformações de imagem.\n",
    "def get_transform(train):\n",
    "    \n",
    "    transforms_list = []\n",
    "    bbox_params = None\n",
    "    \n",
    "    if train:\n",
    "        \n",
    "        # Se for para treinamento, aplica data augmentation.\n",
    "        transforms_list.append(A.HorizontalFlip(p=0.5)) # Espelhamento horizontal com 50% de chance.\n",
    "        transforms_list.append(A.RandomBrightnessContrast(p=0.2)) # Muda brilho e contraste.\n",
    "        \n",
    "        # Define os parâmetros para as bounding boxes, para que elas se ajustem com as augmentações.\n",
    "        bbox_params = A.BboxParams(format='pascal_voc', label_fields=['labels'], min_area=1, min_visibility=0.1)\n",
    "    \n",
    "    # Sempre converte a imagem para um tensor PyTorch.\n",
    "    transforms_list.append(ToTensorV2())\n",
    "    \n",
    "    # Compõe (agrupa) as transformações em um único pipeline.\n",
    "    if bbox_params:\n",
    "        return A.Compose(transforms_list, bbox_params=bbox_params)\n",
    "    \n",
    "    else:\n",
    "        return A.Compose(transforms_list)\n",
    "\n",
    "# Função de agrupamento para o DataLoader.\n",
    "def collate_fn(batch):\n",
    "    \n",
    "    # Filtra amostras que retornaram 'None' do __getitem__.\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    \n",
    "    if not batch:\n",
    "        return None, None\n",
    "    \n",
    "    # Separa as imagens e os alvos em duas tuplas separadas.\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Função para criar o modelo de detecção.\n",
    "def get_detection_model(num_classes):\n",
    "    \n",
    "    # Carrega um modelo Faster R-CNN com backbone ResNet50, pré-treinado no COCO.\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "    \n",
    "    # Obtém o número de características de entrada do classificador.\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Substitui a \"cabeça\" do classificador por uma nova, com o número correto de classes para nosso problema.\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "print(\"Célula 5: Funções Utilitárias definidas (VERSÃO FINAL E CORRIGIDA).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T19:46:09.225558Z",
     "iopub.status.busy": "2025-06-19T19:46:09.22536Z",
     "iopub.status.idle": "2025-06-19T19:46:09.241712Z",
     "shell.execute_reply": "2025-06-19T19:46:09.241163Z",
     "shell.execute_reply.started": "2025-06-19T19:46:09.225544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 6: ENGINE DE TREINAMENTO (VERSÃO CORRIGIDA)\n",
    "# ==============================================================================\n",
    "\n",
    "# Função para treinar o modelo por uma época.\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    \"\"\"\n",
    "    Executa uma única época de treinamento. (Esta função já estava correta)\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()  # Coloca o modelo em modo de treinamento.\n",
    "    prog_bar = tqdm(data_loader, total=len(data_loader), desc=f\"Época {epoch+1} [Treino]\")\n",
    "    train_epoch_loss = 0\n",
    "    \n",
    "    # Itera sobre os lotes de dados do data_loader de treinamento.\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        \n",
    "        if data is None or data[0] is None:\n",
    "            continue  # Pula o lote se for inválido.\n",
    "            \n",
    "        imgs, annotations = data\n",
    "        \n",
    "        # Move as imagens e anotações para o dispositivo (GPU/CPU).\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        \n",
    "        # O modelo retorna um dicionário de perdas (losses) quando está em modo de treino.\n",
    "        loss_dict = model(imgs, annotations)\n",
    "        \n",
    "        # Soma todas as perdas (ex: perda de classificação, perda de regressão da caixa).\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Verificação de segurança: se a perda for infinita ou NaN, pula a atualização.\n",
    "        if not torch.isfinite(losses):\n",
    "            print(f\"ALERTA: Loss infinita na iteração {i}, pulando batch.\")\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()   # Zera os gradientes acumulados.\n",
    "        losses.backward()  \t\t# Calcula os gradientes (backpropagation).\n",
    "        \n",
    "        # Limita a norma dos gradientes para evitar \"exploding gradients\".\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\t# Atualiza os pesos do modelo.\n",
    "        \n",
    "        train_epoch_loss += losses.item()  \t\t \t# Acumula a perda da época.\n",
    "        prog_bar.set_postfix(loss=losses.item())\t# Atualiza a barra de progresso com a loss atual.\n",
    "\n",
    "    # Retorna a média da perda de treinamento da época.\n",
    "    return train_epoch_loss / len(data_loader) if len(data_loader) > 0 else 0.0\n",
    "\n",
    "\n",
    "# --- FUNÇÃO 'evaluate' ---\n",
    "# Decorador que desativa o cálculo de gradientes, economizando memória e acelerando a execução.\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Executa la evaluación del modelo en el conjunto de datos de validación.\n",
    "    \"\"\"\n",
    "    model.eval()\t# Coloca o modelo em modo de avaliação.\n",
    "    \n",
    "    prog_bar = tqdm(data_loader, total=len(data_loader), desc=\"[Validação]\")\n",
    "    validation_loss = 0\n",
    "    \n",
    "    # Itera sobre os lotes de dados do data_loader de validação.\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        if data is None or data[0] is None:\n",
    "            continue\n",
    "\n",
    "        imgs, annotations = data\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        \n",
    "        was_training = model.training\t# Guarda o estado atual (que é 'eval').\n",
    "        model.train()\t# Muda para modo 'train' para obter o dict de loss.\n",
    "        loss_dict = model(imgs, annotations)\n",
    "        model.train(was_training)\t\t# Restaura o estado original ('eval').\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        if torch.isfinite(losses): \n",
    "            validation_loss += losses.item()\n",
    "        \n",
    "        prog_bar.set_postfix(loss=losses.item())\n",
    "        \n",
    "    # Retorna a média da perda de validação.\n",
    "    return validation_loss / len(data_loader) if len(data_loader) > 0 else 0.0\n",
    "\n",
    "print(\"Célula 6: Funções de Engine definidas (com 'evaluate' corrigido).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# (As funções auxiliares como collate_fn, get_transform, etc., já foram definidas nas células anteriores)\n",
    "\n",
    "# --- Função para criar o modelo (versão específica para ResNet-50) ---\n",
    "def get_model_fasterrcnn_resnet50(num_classes):\n",
    "    \"\"\"\n",
    "    Carrega um modelo Faster R-CNN pré-treinado (backbone ResNet-50)\n",
    "    e o adapta para o número de classes desejado.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Carrega o modelo com pesos pré-treinados.\n",
    "    model = fasterrcnn_resnet50_fpn(weights='FasterRCNN_ResNet50_FPN_Weights.DEFAULT')\n",
    "    \n",
    "    # Obtém o número de features da camada de predição.\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Substitui a camada de predição por uma nova, adequada ao nosso número de classes.\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "# --- 1. CONFIGURAÇÃO INICIAL ---\n",
    "print(\"--- SCRIPT DE TREINAMENTO: Faster R-CNN com ResNet-50 ---\")\n",
    "print(f\"Dispositivo selecionado: {config.DEVICE}\")\n",
    "\n",
    "# Define um diretório específico para os checkpoints deste modelo.\n",
    "config.CHECKPOINT_DIR = './checkpoints/fasterrcnn_resnet50'\n",
    "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"Checkpoints serão salvos em: {config.CHECKPOINT_DIR}\")\n",
    "\n",
    "# --- 2. DATASETS E DATALOADERS ---\n",
    "print(\"\\nConfigurando Datasets...\")\n",
    "# Cria o dataset de treinamento usando a classe customizada.\n",
    "dataset_train = FilteredCOCODataset(\n",
    "    root=config.TRAIN_DATA_DIR, \n",
    "    annotation=config.TRAIN_COCO, \n",
    "    transforms=get_transform(train=True), \n",
    "    cats=config.CATEGORIAS_DESEJADAS,\n",
    "    limit=config.IMAGE_LIMIT\n",
    ")\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=config.TRAIN_BATCH_SIZE, shuffle=config.TRAIN_SHUFFLE_DL, num_workers=config.NUM_WORKERS_DL, collate_fn=collate_fn)\n",
    "\n",
    "# Cria o dataset de validação.\n",
    "dataset_val = FilteredCOCODataset(\n",
    "    root=config.VAL_DATA_DIR, \n",
    "    annotation=config.VAL_COCO, \n",
    "    transforms=get_transform(train=False), \n",
    "    cats=config.CATEGORIAS_DESEJADAS,\n",
    "    limit=config.VAL_IMAGE_LIMIT\n",
    ")\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=config.NUM_WORKERS_DL, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Dataset de Treinamento: {len(dataset_train)} imagens.\")\n",
    "print(f\"Dataset de Validação: {len(dataset_val)} imagens.\")\n",
    "\n",
    "# --- 3. MODELO E OTIMIZADOR ---\n",
    "print(\"\\nCarregando modelo...\")\n",
    "model = get_model_fasterrcnn_resnet50(config.NUM_CLASSES)\n",
    "model.to(config.DEVICE) # Move o modelo para a GPU/CPU.\n",
    "\n",
    "# Seleciona os parâmetros que precisam de gradientes para o otimizador.\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=config.LR, momentum=config.MOMENTUM, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "### MODIFICAÇÃO 1: LÓGICA DE RETOMADA SIMPLIFICADA ###\n",
    "# --- 4. LÓGICA DE CHECKPOINT (RETOMADA) ---\n",
    "print(\"\\nProcurando pelo melhor checkpoint para retomar...\")\n",
    "best_val_loss = float('inf') # Inicializa a melhor perda com infinito.\n",
    "start_epoch = 0 # Época inicial é 0.\n",
    "\n",
    "# Procura por um único arquivo de checkpoint, que representa o melhor modelo até agora.\n",
    "best_model_path = os.path.join(config.CHECKPOINT_DIR, \"best_model_checkpoint.pth\")\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    \n",
    "    print(f\"Retomando do melhor checkpoint encontrado: {best_model_path}\")\n",
    "    checkpoint = torch.load(best_model_path) \t# Carrega o checkpoint.\n",
    "    \n",
    "    # Restaura o estado do modelo, do otimizador e outros dados do treinamento.\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] \t# Define a época de início para a próxima.\n",
    "    best_val_loss = checkpoint.get('best_val_loss', float('inf')) \n",
    "    \n",
    "    print(f\"Retomado da época {start_epoch}. Melhor loss de validação até agora: {best_val_loss:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Nenhum checkpoint encontrado. Iniciando do zero.\")\n",
    "\n",
    "\n",
    "### MODIFICAÇÃO 2: LOOP DE TREINAMENTO SIMPLIFICADO ###\n",
    "# --- 5. LOOP DE TREINAMENTO PRINCIPAL ---\n",
    "print(\"\\n--- Iniciando Treinamento ---\")\n",
    "for epoch in range(start_epoch, config.NUM_EPOCHS):\n",
    "    \n",
    "    # Executa uma época de treino e uma de validação.\n",
    "    avg_train_loss = train_one_epoch(model, optimizer, data_loader_train, config.DEVICE, epoch)\n",
    "    avg_val_loss = evaluate(model, data_loader_val, config.DEVICE)\n",
    "    print(f\"--- Fim da Época {epoch+1}/{config.NUM_EPOCHS} | Loss Treino: {avg_train_loss:.4f} | Loss Validação: {avg_val_loss:.4f} ---\")\n",
    "\n",
    "    # Lógica para salvar o melhor modelo.\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        \n",
    "        best_val_loss = avg_val_loss\n",
    "        \n",
    "        # O nome do arquivo é fixo, então ele será sempre sobrescrito com um modelo melhor.\n",
    "        best_checkpoint_path = os.path.join(config.CHECKPOINT_DIR, \"best_model_checkpoint.pth\")\n",
    "        \n",
    "        print(f\"🎉 Nova melhor loss de VALIDAÇÃO! Salvando checkpoint em: {best_checkpoint_path}\")\n",
    "\n",
    "        # Salva um dicionário completo para permitir a retomada do treinamento.\n",
    "        checkpoint_data = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss\n",
    "        }\n",
    "        torch.save(checkpoint_data, best_checkpoint_path)\n",
    "\n",
    "    print(\"-\" * 50) # Separador visual entre as épocas.\n",
    "\n",
    "print(\"--- Treinamento Concluído ---\")\n",
    "print(f\"O melhor modelo (com loss de validação {best_val_loss:.4f}) está salvo em '{best_model_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T19:46:47.388824Z",
     "iopub.status.busy": "2025-06-19T19:46:47.388229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_mobilenet.py\n",
    "\n",
    "# --- Função para criar o modelo ---\n",
    "def get_model_fasterrcnn_mobilenet(num_classes):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cria um modelo Faster R-CNN usando um backbone MobileNetV2.\n",
    "    Este é um modelo mais leve que o ResNet-50.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Carrega o extrator de features (backbone) do MobileNetV2 pré-treinado no ImageNet.\n",
    "    backbone = torchvision.models.mobilenet_v2(weights='MobileNet_V2_Weights.DEFAULT').features\n",
    "    \n",
    "    # O número de canais de saída do backbone é necessário para a próxima camada.\n",
    "    backbone.out_channels = 1280\n",
    "    \n",
    "    # Define o gerador de âncoras para a Region Proposal Network (RPN).\n",
    "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "    \n",
    "    # Define a camada de RoI pooling.\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)\n",
    "    \n",
    "    # Constrói o modelo Faster R-CNN com os componentes definidos.\n",
    "    model = FasterRCNN(backbone, num_classes=num_classes, rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler)\n",
    "    return model\n",
    "\n",
    "# --- 1. CONFIGURAÇÃO INICIAL ---\n",
    "print(\"--- SCRIPT DE TREINAMENTO: Faster R-CNN com MobileNetV2 ---\")\n",
    "print(f\"Dispositivo selecionado: {config.DEVICE}\")\n",
    "\n",
    "# Diretório de checkpoint específico para este modelo.\n",
    "config.CHECKPOINT_DIR = './checkpoints/fasterrcnn_mobilenetv2'\n",
    "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoints serão salvos em: {config.CHECKPOINT_DIR}\")\n",
    "\n",
    "# --- 2. DATASETS E DATALOADERS ---\n",
    "# (Esta seção é idêntica à do script ResNet-50)\n",
    "print(\"\\nConfigurando Datasets...\")\n",
    "\n",
    "dataset_train = FilteredCOCODataset(root=config.TRAIN_DATA_DIR, annotation=config.TRAIN_COCO, transforms=get_transform(train=True), cats=config.CATEGORIAS_DESEJADAS, limit=config.IMAGE_LIMIT)\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=config.TRAIN_BATCH_SIZE, shuffle=config.TRAIN_SHUFFLE_DL, num_workers=config.NUM_WORKERS_DL, collate_fn=collate_fn)\n",
    "\n",
    "dataset_val = FilteredCOCODataset(root=config.VAL_DATA_DIR, annotation=config.VAL_COCO, transforms=get_transform(train=False), cats=config.CATEGORIAS_DESEJADAS, limit=config.VAL_IMAGE_LIMIT)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=config.NUM_WORKERS_DL, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Dataset de Treinamento: {len(dataset_train)} imagens.\")\n",
    "print(f\"Dataset de Validação: {len(dataset_val)} imagens.\")\n",
    "\n",
    "# --- 3. MODELO E OTIMIZADOR ---\n",
    "print(\"\\nCarregando modelo...\")\n",
    "\n",
    "model = get_model_fasterrcnn_mobilenet(config.NUM_CLASSES)\n",
    "model.to(config.DEVICE)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=config.LR, momentum=config.MOMENTUM, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "# --- 4. LÓGICA DE CHECKPOINT (RETOMADA) ---\n",
    "best_val_loss = float('inf')\n",
    "start_epoch = 0\n",
    "# Procura por todos os checkpoints salvos anteriormente.\n",
    "checkpoint_files = glob.glob(os.path.join(config.CHECKPOINT_DIR, 'checkpoint_epoch_*.pth'))\n",
    "\n",
    "if checkpoint_files:\n",
    "    \n",
    "    # Extrai o número da época do nome de cada arquivo.\n",
    "    epochs = [int(re.search(r'(\\d+)', os.path.basename(f)).group(1)) for f in checkpoint_files]\n",
    "    latest_epoch = max(epochs) # Encontra a época mais recente.\n",
    "    latest_checkpoint_path = os.path.join(config.CHECKPOINT_DIR, f'checkpoint_epoch_{latest_epoch}.pth')\n",
    "    \n",
    "    print(f\"\\nRetomando do checkpoint: {latest_checkpoint_path}\")\n",
    "    \n",
    "    checkpoint = torch.load(latest_checkpoint_path)\n",
    "    \n",
    "    # Restaura o estado do modelo, otimizador e outros dados.\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_val_loss = checkpoint.get('best_val_loss', float('inf')) \n",
    "    \n",
    "    print(f\"Retomado da época {start_epoch}. Melhor loss de validação: {best_val_loss:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNenhum checkpoint encontrado. Iniciando do zero.\")\n",
    "\n",
    "# --- 5. LOOP DE TREINAMENTO PRINCIPAL ---\n",
    "print(\"\\n--- Iniciando Treinamento ---\")\n",
    "for epoch in range(start_epoch, config.NUM_EPOCHS):\n",
    "    \n",
    "    avg_train_loss = train_one_epoch(model, optimizer, data_loader_train, config.DEVICE, epoch)\n",
    "    avg_val_loss = evaluate(model, data_loader_val, config.DEVICE)\n",
    "    \n",
    "    print(f\"--- Fim da Época {epoch+1}/{config.NUM_EPOCHS} | Loss Treino: {avg_train_loss:.4f} | Loss Validação: {avg_val_loss:.4f} ---\")\n",
    "\n",
    "    # Se a loss de validação atual for a melhor até agora...\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        \n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_path = os.path.join(config.CHECKPOINT_DIR, \"best_model.pth\")\n",
    "        # ...salva apenas os pesos do modelo (state_dict) no arquivo 'best_model.pth'.\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        \n",
    "        print(f\"🎉 Nova melhor loss de VALIDAÇÃO! Modelo salvo em: {best_model_path}\")\n",
    "\n",
    "    # Salva um checkpoint completo ao final de CADA época.\n",
    "    checkpoint_path = os.path.join(config.CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "    checkpoint = {'epoch': epoch + 1, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'train_loss': avg_train_loss, 'val_loss': avg_val_loss, 'best_val_loss': best_val_loss}\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    print(f\"Checkpoint da época {epoch+1} salvo.\\n\")\n",
    "\n",
    "print(\"--- Treinamento Concluído ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-19T00:47:19.38Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_efficientnet.py (Versão que salva apenas o melhor modelo)\n",
    "\n",
    "# --- Função para criar o modelo ---\n",
    "def get_model_fasterrcnn_efficientnet(num_classes):\n",
    "    \"\"\"\n",
    "    Cria um modelo Faster R-CNN usando um backbone EfficientNet-B0.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Carrega o extrator de features (backbone) do EfficientNet-B0 pré-treinado.\n",
    "    backbone = torchvision.models.efficientnet_b0(weights='EfficientNet_B0_Weights.DEFAULT').features\n",
    "    \n",
    "    # Define manualmente o número de canais de saída do backbone.\n",
    "    backbone.out_channels = 1280\n",
    "    \n",
    "    # Define o gerador de âncoras e o RoI Pooler, similar ao MobileNet.\n",
    "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)\n",
    "    \n",
    "    # Constrói o modelo Faster R-CNN final.\n",
    "    model = FasterRCNN(backbone, num_classes=num_classes, rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# --- 1. CONFIGURAÇÃO INICIAL ---\n",
    "print(\"--- SCRIPT DE TREINAMENTO: Faster R-CNN com EfficientNet-B0 ---\")\n",
    "print(f\"Dispositivo selecionado: {config.DEVICE}\")\n",
    "\n",
    "# Diretório de checkpoint específico para este modelo.\n",
    "config.CHECKPOINT_DIR = './checkpoints/fasterrcnn_efficientnetb0'\n",
    "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoints serão salvos em: {config.CHECKPOINT_DIR}\")\n",
    "\n",
    "# --- 2. DATASETS E DATALOADERS ---\n",
    "# (Esta seção é idêntica à dos outros scripts)\n",
    "print(\"\\nConfigurando Datasets...\")\n",
    "dataset_train = FilteredCOCODataset(root=config.TRAIN_DATA_DIR, annotation=config.TRAIN_COCO, transforms=get_transform(train=True), cats=config.CATEGORIAS_DESEJADAS, limit=config.IMAGE_LIMIT)\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=config.TRAIN_BATCH_SIZE, shuffle=config.TRAIN_SHUFFLE_DL, num_workers=config.NUM_WORKERS_DL, collate_fn=collate_fn)\n",
    "\n",
    "dataset_val = FilteredCOCODataset(root=config.VAL_DATA_DIR, annotation=config.VAL_COCO, transforms=get_transform(train=False), cats=config.CATEGORIAS_DESEJADAS, limit=config.VAL_IMAGE_LIMIT)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=config.NUM_WORKERS_DL, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Dataset de Treinamento: {len(dataset_train)} imagens.\")\n",
    "print(f\"Dataset de Validação: {len(dataset_val)} imagens.\")\n",
    "\n",
    "# --- 3. MODELO E OTIMIZADOR ---\n",
    "print(\"\\nCarregando modelo...\")\n",
    "model = get_model_fasterrcnn_efficientnet(config.NUM_CLASSES)\n",
    "model.to(config.DEVICE)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=config.LR, momentum=config.MOMENTUM, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "### MODIFICAÇÃO 1: LÓGICA DE RETOMADA SIMPLIFICADA ###\n",
    "# --- 4. LÓGICA DE CHECKPOINT (RETOMADA) ---\n",
    "# (Esta seção é idêntica à do script ResNet-50)\n",
    "print(\"\\nProcurando pelo melhor checkpoint para retomar...\")\n",
    "best_val_loss = float('inf')\n",
    "start_epoch = 0\n",
    "\n",
    "# Procura pelo arquivo único do melhor modelo.\n",
    "best_model_path = os.path.join(config.CHECKPOINT_DIR, \"best_model_checkpoint.pth\")\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    \n",
    "    print(f\"Retomando do melhor checkpoint encontrado: {best_model_path}\")\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    \n",
    "    # Restaura o estado completo do treinamento.\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_val_loss = checkpoint.get('best_val_loss', float('inf')) \n",
    "    \n",
    "    print(f\"Retomado da época {start_epoch}. Melhor loss de validação até agora: {best_val_loss:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Nenhum checkpoint encontrado. Iniciando do zero.\")\n",
    "\n",
    "\n",
    "### MODIFICAÇÃO 2: LOOP DE TREINAMENTO SIMPLIFICADO ###\n",
    "# --- 5. LOOP DE TREINAMENTO PRINCIPAL ---\n",
    "# (Esta seção é idêntica à do script ResNet-50)\n",
    "print(\"\\n--- Iniciando Treinamento ---\")\n",
    "for epoch in range(start_epoch, config.NUM_EPOCHS):\n",
    "    \n",
    "    avg_train_loss = train_one_epoch(model, optimizer, data_loader_train, config.DEVICE, epoch)\n",
    "    avg_val_loss = evaluate(model, data_loader_val, config.DEVICE)\n",
    "    print(f\"--- Fim da Época {epoch+1}/{config.NUM_EPOCHS} | Loss Treino: {avg_train_loss:.4f} | Loss Validação: {avg_val_loss:.4f} ---\")\n",
    "\n",
    "    # Se a loss de validação for a melhor, salva um checkpoint completo e sobrescreve o anterior.\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        \n",
    "        best_val_loss = avg_val_loss\n",
    "        best_checkpoint_path = os.path.join(config.CHECKPOINT_DIR, \"best_model_checkpoint.pth\")\n",
    "        print(f\"🎉 Nova melhor loss de VALIDAÇÃO! Salvando checkpoint em: {best_checkpoint_path}\")\n",
    "\n",
    "        checkpoint_data = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss\n",
    "        }\n",
    "        torch.save(checkpoint_data, best_checkpoint_path)\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"--- Treinamento Concluído ---\")\n",
    "print(f\"O melhor modelo (com loss de validação {best_val_loss:.4f}) está salvo em '{best_model_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Célula 10: Avaliação Quantitativa (mAP)\n",
    "\n",
    "# Função para rodar a avaliação de mAP (Mean Average Precision).\n",
    "def run_map_evaluation(weights_path):\n",
    "    \n",
    "    print(\"\\n--- Iniciando Avaliação de Métricas mAP ---\")\n",
    "    \n",
    "    # Verifica se o data_loader de validação existe.\n",
    "    if 'data_loader_val' not in locals():\n",
    "        \n",
    "        print(\"DataLoader de validação não encontrado. Execute a célula de treinamento primeiro.\")\n",
    "        return\n",
    "\n",
    "    # Cria uma nova instância do modelo para avaliação.\n",
    "    eval_model = get_detection_model(config.NUM_CLASSES)\n",
    "    \n",
    "    # Carrega os pesos do melhor modelo treinado.\n",
    "    eval_model.load_state_dict(torch.load(weights_path, map_location=config.DEVICE))\n",
    "    eval_model.to(config.DEVICE)\n",
    "    eval_model.eval() # Coloca o modelo em modo de avaliação.\n",
    "    \n",
    "    # Inicializa o objeto de métrica da torchmetrics para calcular o mAP.\n",
    "    # box_format='xyxy' corresponde ao formato [xmin, ymin, xmax, ymax].\n",
    "    metric = MeanAveragePrecision(box_format='xyxy', iou_type=\"bbox\")\n",
    "    prog_bar = tqdm(data_loader_val, total=len(data_loader_val), desc=\"Calculando Métricas mAP...\")\n",
    "    \n",
    "    # Itera sobre o dataset de validação.\n",
    "    for images, targets in prog_bar:\n",
    "        \n",
    "        images = list(img.to(config.DEVICE) for img in images)\n",
    "        \n",
    "        # O modelo em modo 'eval' retorna uma lista de predições.\n",
    "        predictions = eval_model(images)\n",
    "        \n",
    "        # Formata os alvos (ground truth) para o formato esperado pela métrica.\n",
    "        formatted_targets = [{\"boxes\": t[\"boxes\"], \"labels\": t[\"labels\"]} for t in targets]\n",
    "        \n",
    "        # Atualiza a métrica com as predições e os alvos do lote atual.\n",
    "        metric.update(predictions, formatted_targets)\n",
    "        \n",
    "    # Calcula os resultados finais da métrica acumulada.\n",
    "    results = metric.compute()\n",
    "    print(\"\\n--- Resultados da Avaliação (mAP para Bounding Box) ---\")\n",
    "    \n",
    "    # Imprime os resultados de forma organizada.\n",
    "    for key, value in results.items():\n",
    "        \n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"{key:<25}: {value.item():.4f}\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "\n",
    "# Executa a avaliação no melhor modelo salvo pelo script de treinamento.\n",
    "# Nota: Este caminho pode precisar de ajuste dependendo de qual script de treino foi executado.\n",
    "best_model_file = os.path.join(config.CHECKPOINT_DIR, 'best_model.pth')\n",
    "if os.path.exists(best_model_file):\n",
    "    run_map_evaluation(best_model_file)\n",
    "    \n",
    "else:\n",
    "    print(f\"Arquivo '{best_model_file}' não encontrado. Execute o treinamento primeiro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Célula 11: Teste Prático em uma Imagem\n",
    "\n",
    "# Função para testar o modelo em uma única imagem e visualizar as detecções.\n",
    "def test_single_image(weights_path, image_path, threshold=0.5):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"ERRO: Arquivo de imagem não encontrado em: {image_path}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Iniciando Teste Prático ---\")\n",
    "    \n",
    "    # Cria uma instância do modelo e carrega os pesos treinados.\n",
    "    test_model = get_detection_model(config.NUM_CLASSES)\n",
    "    test_model.load_state_dict(torch.load(weights_path, map_location=config.DEVICE))\n",
    "    test_model.to(config.DEVICE)\n",
    "    test_model.eval() # Modo de avaliação.\n",
    "\n",
    "    # Carrega a imagem de teste usando PIL e converte para RGB.\n",
    "    image_pil = Image.open(image_path).convert('RGB')\n",
    "    # Converte a imagem PIL para um tensor PyTorch.\n",
    "    image_tensor = F.to_tensor(image_pil)\n",
    "    \n",
    "    # Realiza a predição sem calcular gradientes.\n",
    "    with torch.no_grad():\n",
    "        # O modelo espera uma lista de imagens, por isso [image_tensor].\n",
    "        prediction = test_model([image_tensor.to(config.DEVICE)])[0]\n",
    "\n",
    "    # Converte a imagem PIL para o formato OpenCV (BGR) para desenhar nela.\n",
    "    image_cv = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Define os nomes das classes para exibição.\n",
    "    CLASS_NAMES = ['background'] + config.CATEGORIAS_DESEJADAS\n",
    "    # Gera cores aleatórias para cada classe.\n",
    "    colors = [[random.randint(0, 255) for _ in range(3)] for _ in CLASS_NAMES]\n",
    "\n",
    "    # Filtra as predições com base no limiar de confiança (score).\n",
    "    scores = prediction['scores'].cpu().numpy()\n",
    "    high_confidence_indices = scores > threshold\n",
    "    boxes = prediction['boxes'][high_confidence_indices].cpu().numpy().astype(int)\n",
    "    labels = prediction['labels'][high_confidence_indices].cpu().numpy()\n",
    "\n",
    "    print(f\"Encontrados {len(boxes)} objetos com confiança > {threshold}\")\n",
    "\n",
    "    # Itera sobre os objetos detectados para desenhar na imagem.\n",
    "    for i in range(len(boxes)):\n",
    "        box = boxes[i]\n",
    "        label_id = labels[i]\n",
    "        class_name = CLASS_NAMES[label_id]\n",
    "        color = colors[label_id]\n",
    "        score = scores[i]\n",
    "        text = f\"{class_name}: {score:.2f}\" # Texto com nome da classe e score.\n",
    "        \n",
    "        # Desenha o retângulo (bounding box).\n",
    "        cv2.rectangle(image_cv, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
    "        # Calcula o tamanho do texto para criar um fundo.\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
    "        # Desenha um retângulo de fundo para o texto.\n",
    "        cv2.rectangle(image_cv, (box[0], box[1] - text_height - 5), (box[0] + text_width, box[1] - 5), color, -1)\n",
    "        # Escreve o texto.\n",
    "        cv2.putText(image_cv, text, (box[0], box[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1)\n",
    "\n",
    "    # Exibe a imagem final com as detecções usando Matplotlib.\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Converte a imagem de volta para RGB para exibição correta no Matplotlib.\n",
    "    plt.imshow(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off') # Remove os eixos.\n",
    "    plt.show()\n",
    "\n",
    "# --- PARA USAR ESTA CÉLULA ---\n",
    "# 1. Faça upload de uma imagem de teste para o seu notebook.\n",
    "# 2. Atualize os caminhos abaixo.\n",
    "# Caminho para o melhor modelo salvo. Verifique se o nome/caminho está correto.\n",
    "best_model_file = os.path.join(config.CHECKPOINT_DIR, 'best_model.pth')\n",
    "# Caminho para a sua imagem de teste.\n",
    "test_image_path = \"/kaggle/input/coco-dataset/my_data/val/000000000785.jpg\" # <--- MUDE AQUI PARA SUA IMAGEM\n",
    "\n",
    "# Verifica se os arquivos existem antes de tentar rodar o teste.\n",
    "if os.path.exists(best_model_file) and os.path.exists(test_image_path):\n",
    "    test_single_image(best_model_file, test_image_path, threshold=0.5)\n",
    "else:\n",
    "    print(\"Arquivo 'best_model.pth' ou imagem de teste não encontrados. Execute o treinamento primeiro e verifique o caminho da imagem.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7684498,
     "sourceId": 12199220,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
