{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":6019472,"sourceType":"datasetVersion","datasetId":3445072},{"sourceId":12262703,"sourceType":"datasetVersion","datasetId":7727262},{"sourceId":447211,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":363081,"modelId":383946},{"sourceId":447212,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":363082,"modelId":383947},{"sourceId":447213,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":363083,"modelId":383948}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Este notebook realiza uma validação em um agrupamento de dados diferente, ainda pertencente ao dataset COCO, utilizando o grupo de validação do COCO 2014.\n\nIsso permite avaliar a capacidade de generalização do modelo em dados não vistos durante o treinamento, mas que fazem parte do mesmo universo do COCO.","metadata":{}},{"cell_type":"code","source":"# Instalação de Dependências\n\n!pip install -q tqdm torchmetrics opencv-python\n!pip install -U albumentations\n\nprint(\"Dependências prontas.\")","metadata":{"execution":{"iopub.status.busy":"2025-06-24T12:54:44.851006Z","iopub.execute_input":"2025-06-24T12:54:44.851431Z","iopub.status.idle":"2025-06-24T12:54:55.097958Z","shell.execute_reply.started":"2025-06-24T12:54:44.851392Z","shell.execute_reply":"2025-06-24T12:54:55.094648Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.8)\nRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.26.4)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.15.2)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\nRequirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.4)\nRequirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\nRequirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.3)\nRequirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.13.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->albumentations) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.24.4->albumentations) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.24.4->albumentations) (2024.2.0)\nDependências prontas.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Imports Globais\n\n# --- PyTorch e Torchvision ---\nimport torch\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- Manipulação de Imagens e Dados ---\nfrom PIL import Image  \t# Biblioteca para abrir e manipular imagens (Pillow).\nimport numpy as np\nimport albumentations as A\t# Biblioteca para data augmentation.\nfrom pycocotools.coco import COCO\t# Utilitário para manusear anotações COCO.\nfrom albumentations.pytorch import ToTensorV2\t# Converte imagens (numpy/pil) para tensores PyTorch.\n\n# --- Utilitários de Treinamento e Avaliação ---\nfrom tqdm.auto import tqdm\t# Cria barras de progresso que funcionam bem em notebooks.\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision  # Métrica padrão para detecção de objetos.\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.transforms import functional as F # torchvision para usar a função to_tensor.\n\n# --- Bibliotecas Padrão do Python ---\nimport os \t\t# Para manipulação de caminhos e arquivos.\nimport glob  \t# Para encontrar arquivos\nimport re  \t\t# Para usar expressões regulares\nimport random\t# Para operações de aleatoriedade\nimport cv2  \t# OpenCV, para desenhar caixas e texto nas imagens de teste.\nimport matplotlib.pyplot as plt # Para exibir imagens no notebook.","metadata":{"execution":{"iopub.status.busy":"2025-06-24T12:54:55.101585Z","iopub.execute_input":"2025-06-24T12:54:55.102545Z","iopub.status.idle":"2025-06-24T12:55:07.514777Z","shell.execute_reply.started":"2025-06-24T12:54:55.102458Z","shell.execute_reply":"2025-06-24T12:55:07.511894Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Célula 4: Configuração\n\n# Define uma classe para centralizar todas as configurações do projeto.\nclass Config:\n    \n    # --- CAMINHO PARA O DATASET ---\n    DATA_ROOT_PATH = '/kaggle/input/coco-dataset/my_data'\n    \n    # --- CONFIGURAÇÕES GERAIS ---\n    # Define o dispositivo de computação: 'cuda' (GPU) se disponível, senão 'cpu'.\n    DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    \n    # Diretório onde os checkpoints do modelo serão salvos durante o treinamento.\n    CHECKPOINT_DIR = '/kaggle/working/checkpoints/'\n    \n    # Define o número total de classes. Como o modelo precisa de uma classe para o fundo (background),\n    # o número é (quantidade de classes reais + 1).\n    NUM_CLASSES = 4  # 3 classes (person, car, dog) + 1 background\n    \n    # Lista com os nomes das categorias que queremos extrair do dataset COCO.\n    CATEGORIAS_DESEJADAS = ['person', 'car', 'dog']\n    \n    # --- CONFIGURAÇÕES DO DATASET ---\n    # Limita o número de imagens para acelerar o treinamento e a validação.\n    IMAGE_LIMIT = 3000  \t# Limite para o conjunto de treino.\n    VAL_IMAGE_LIMIT = 300 \t# Limite para o conjunto de validação.\n\n    # Caminhos específicos para os dados de treino e validação e seus arquivos de anotação JSON.\n    TRAIN_DATA_DIR = os.path.join(DATA_ROOT_PATH, 'train/')\n    TRAIN_COCO = os.path.join(DATA_ROOT_PATH, 'annotations/instances_train2017.json')\n    VAL_DATA_DIR = os.path.join(DATA_ROOT_PATH, 'val/')\n    VAL_COCO = os.path.join(DATA_ROOT_PATH, 'annotations/instances_val2017.json')\n\n    # --- HIPERPARÂMETROS DE TREINAMENTO ---\n    NUM_EPOCHS = 20  \t\t# Número total de épocas para treinar o modelo.\n    TRAIN_BATCH_SIZE = 1 \t# Quantidade de imagens por lote de treinamento.\n    TRAIN_SHUFFLE_DL = True\t# Embaralhar o dataset de treino a cada época.\n    NUM_WORKERS_DL = 0  \t# Número de processos para carregar dados. 0 significa que será na thread principal.\n\n    # Parâmetros do otimizador SGD (Gradiente Descendente Estocástico).\n    LR = 0.001  \t# Taxa de aprendizado (learning rate).\n    MOMENTUM = 0.9  # Momento, ajuda a acelerar o SGD na direção certa.\n    WEIGHT_DECAY = 0.0005  # Termo de regularização L2 para evitar overfitting.\n\n# Cria uma instância da classe de configuração para ser usada no restante do código.\nconfig = Config()\nprint(\"Configurações definidas.\")","metadata":{"execution":{"iopub.status.busy":"2025-06-24T12:55:07.518067Z","iopub.execute_input":"2025-06-24T12:55:07.519445Z","iopub.status.idle":"2025-06-24T12:55:07.537431Z","shell.execute_reply.started":"2025-06-24T12:55:07.519380Z","shell.execute_reply":"2025-06-24T12:55:07.535562Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Configurações definidas.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ==============================================================================\n# CÉLULA 5: FUNÇÕES UTILITÁRIAS (VERSÃO FINAL E CORRIGIDA)\n# Esta versão contém a correção definitiva para o problema de tipo de dado.\n# ==============================================================================\n\n# Classe customizada de Dataset para o formato COCO, com filtros.\nclass FilteredCOCODataset(Dataset):\n    \n    def __init__(self, root, annotation, transforms=None, cats=None, limit=None):\n        \n        self.root = root  \t# Diretório das imagens.\n        self.transforms = transforms  # Transformações de data augmentation.\n        self.coco = COCO(annotation)  # Carrega o arquivo de anotações COCO.\n        \n        # Pega os IDs numéricos das categorias desejadas (ex: 'person', 'car').\n        self.desired_cat_ids = set(self.coco.getCatIds(catNms=cats if cats else []))\n        \n        # Mapeia os IDs originais do COCO para os IDs do nosso modelo (1, 2, 3...).\n        self.coco_to_model_map = {coco_id: i + 1 for i, coco_id in enumerate(sorted(list(self.desired_cat_ids)))}\n        \n        # Lógica para limitar e balancear o dataset.\n        if limit and limit > 0 and len(self.desired_cat_ids) > 0:\n            \n            # Obtém uma lista de imagens para cada categoria desejada.\n            imgs_per_cat = {cat_id: self.coco.getImgIds(catIds=[cat_id]) for cat_id in self.desired_cat_ids}\n            \n            # Calcula quantas imagens pegar por categoria para atingir o limite.\n            limit_per_cat = int(limit / len(self.desired_cat_ids))\n            balanced_img_ids = set() \t# Usa um conjunto para evitar duplicatas.\n            \n            for cat_id in self.desired_cat_ids:\n                \n                image_list = imgs_per_cat[cat_id]\n                random.shuffle(image_list)  # Embaralha para pegar uma amostra aleatória.\n                balanced_img_ids.update(image_list[:limit_per_cat])\n                \n            final_ids = list(balanced_img_ids)\n            random.shuffle(final_ids) # Embaralha a lista final de IDs.\n            \n            # Garante que não passamos do limite e ordena os IDs.\n            self.ids = sorted(final_ids[:limit])\n            \n        else:\n            \n            # Se não houver limite, pega todas as imagens que contêm as categorias desejadas.\n            all_image_ids = set()\n            \n            for cat_id in self.desired_cat_ids:\n                all_image_ids.update(self.coco.getImgIds(catIds=[cat_id]))\n                \n            self.ids = list(sorted(list(all_image_ids)))\n\n    # Método que carrega e retorna um único item (imagem e anotações) do dataset.\n    def __getitem__(self, idx):\n        \n        img_id = self.ids[idx] # Pega o ID da imagem pelo índice.\n        \n        try:\n            \n            # Carrega informações da imagem e suas anotações do COCO.\n            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n            coco_annotations = self.coco.loadAnns(ann_ids)\n            path = self.coco.loadImgs(img_id)[0]['file_name']\n            img = Image.open(os.path.join(self.root, path)).convert('RGB')\n            \n            boxes, labels = [], []\n            \n            for obj in coco_annotations:\n                \n                # Filtra apenas os objetos das categorias desejadas e com área válida.\n                if obj['category_id'] in self.desired_cat_ids and obj['bbox'][2] > 0 and obj['bbox'][3] > 0:\n                    \n                    boxes.append(obj['bbox']) # Bbox no formato [x, y, width, height].\n                    labels.append(self.coco_to_model_map[obj['category_id']]) # Usa o label mapeado.\n            \n            if not boxes: return None # Pula a imagem se não tiver objetos de interesse.\n\n            # Converte as bboxes para o formato [xmin, ymin, xmax, ymax] exigido pelo PyTorch.\n            boxes_tensor = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n            boxes_tensor[:, 2:] += boxes_tensor[:, :2]\n            \n            # Cria o dicionário 'target' com as anotações formatadas.\n            target = {'boxes': boxes_tensor, 'labels': torch.as_tensor(labels, dtype=torch.int64)}\n\n            # Aplica as transformações (data augmentation).\n            if self.transforms:\n                \n                transformed = self.transforms(image=np.array(img), bboxes=target['boxes'].numpy(), labels=target['labels'].numpy())\n                img = transformed['image']\n                if len(transformed['bboxes']) == 0: return None \t# Pula se a augmentação removeu todas as bboxes.\n                target['boxes'] = torch.as_tensor(transformed['bboxes'], dtype=torch.float32).reshape(-1, 4)\n                target['labels'] = torch.as_tensor(transformed['labels'], dtype=torch.int64)\n            \n            # --- SOLUÇÃO DEFINITIVA ---\n            # Garante que a imagem seja um tensor de ponto flutuante (float) e normalizada para o intervalo [0, 1].\n            # ToTensorV2 deveria fazer isso, mas essa conversão manual garante a consistência.\n            if not img.is_floating_point():\n                img = img.to(torch.float32) / 255.0\n\n            return img, target\n        \n        except Exception:\n            \n            # Retorna None se houver qualquer erro ao processar a imagem (ex: arquivo corrompido).\n            return None\n\n    # Método que retorna o número total de amostras no dataset.\n    def __len__(self):\n        \n        return len(self.ids)\n\n# Função que define a sequência de transformações de imagem.\ndef get_transform(train):\n    \n    transforms_list = []\n    bbox_params = None\n    \n    if train:\n        \n        # Se for para treinamento, aplica data augmentation.\n        transforms_list.append(A.HorizontalFlip(p=0.5)) # Espelhamento horizontal com 50% de chance.\n        transforms_list.append(A.RandomBrightnessContrast(p=0.2)) # Muda brilho e contraste.\n        \n        # Define os parâmetros para as bounding boxes, para que elas se ajustem com as augmentações.\n        bbox_params = A.BboxParams(format='pascal_voc', label_fields=['labels'], min_area=1, min_visibility=0.1)\n    \n    # Sempre converte a imagem para um tensor PyTorch.\n    transforms_list.append(ToTensorV2())\n    \n    # Compõe (agrupa) as transformações em um único pipeline.\n    if bbox_params:\n        return A.Compose(transforms_list, bbox_params=bbox_params)\n    \n    else:\n        return A.Compose(transforms_list)\n\n# Função de agrupamento para o DataLoader.\ndef collate_fn(batch):\n    \n    # Filtra amostras que retornaram 'None' do __getitem__.\n    batch = [b for b in batch if b is not None]\n    \n    if not batch:\n        return None, None\n    \n    # Separa as imagens e os alvos em duas tuplas separadas.\n    return tuple(zip(*batch))\n\n# Função para criar o modelo de detecção.\ndef get_detection_model(num_classes):\n    \n    # Carrega um modelo Faster R-CNN com backbone ResNet50, pré-treinado no COCO.\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n    \n    # Obtém o número de características de entrada do classificador.\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    \n    # Substitui a \"cabeça\" do classificador por uma nova, com o número correto de classes para nosso problema.\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model\n\nprint(\"Célula 5: Funções Utilitárias definidas (VERSÃO FINAL E CORRIGIDA).\")","metadata":{"execution":{"iopub.status.busy":"2025-06-24T12:55:07.539615Z","iopub.execute_input":"2025-06-24T12:55:07.540312Z","iopub.status.idle":"2025-06-24T12:55:07.589676Z","shell.execute_reply.started":"2025-06-24T12:55:07.540253Z","shell.execute_reply":"2025-06-24T12:55:07.587634Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Célula 5: Funções Utilitárias definidas (VERSÃO FINAL E CORRIGIDA).\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ==============================================================================\n# CÉLULA 6: ENGINE DE TREINAMENTO (VERSÃO CORRIGIDA)\n# ==============================================================================\n\n# Função para treinar o modelo por uma época.\ndef train_one_epoch(model, optimizer, data_loader, device, epoch):\n    \"\"\"\n    Executa uma única época de treinamento. (Esta função já estava correta)\n    \"\"\"\n    \n    model.train()  # Coloca o modelo em modo de treinamento.\n    prog_bar = tqdm(data_loader, total=len(data_loader), desc=f\"Época {epoch+1} [Treino]\")\n    train_epoch_loss = 0\n    \n    # Itera sobre os lotes de dados do data_loader de treinamento.\n    for i, data in enumerate(prog_bar):\n        \n        if data is None or data[0] is None:\n            continue  # Pula o lote se for inválido.\n            \n        imgs, annotations = data\n        \n        # Move as imagens e anotações para o dispositivo (GPU/CPU).\n        imgs = list(img.to(device) for img in imgs)\n        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n        \n        # O modelo retorna um dicionário de perdas (losses) quando está em modo de treino.\n        loss_dict = model(imgs, annotations)\n        \n        # Soma todas as perdas (ex: perda de classificação, perda de regressão da caixa).\n        losses = sum(loss for loss in loss_dict.values())\n        \n        # Verificação de segurança: se a perda for infinita ou NaN, pula a atualização.\n        if not torch.isfinite(losses):\n            print(f\"ALERTA: Loss infinita na iteração {i}, pulando batch.\")\n            continue\n\n        optimizer.zero_grad()   # Zera os gradientes acumulados.\n        losses.backward()  \t\t# Calcula os gradientes (backpropagation).\n        \n        # Limita a norma dos gradientes para evitar \"exploding gradients\".\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\t# Atualiza os pesos do modelo.\n        \n        train_epoch_loss += losses.item()  \t\t \t# Acumula a perda da época.\n        prog_bar.set_postfix(loss=losses.item())\t# Atualiza a barra de progresso com a loss atual.\n\n    # Retorna a média da perda de treinamento da época.\n    return train_epoch_loss / len(data_loader) if len(data_loader) > 0 else 0.0\n\n\n# --- FUNÇÃO 'evaluate' ---\n# Decorador que desativa o cálculo de gradientes, economizando memória e acelerando a execução.\n@torch.no_grad()\ndef evaluate(model, data_loader, device):\n    \"\"\"\n    Executa la evaluación del modelo en el conjunto de datos de validación.\n    \"\"\"\n    model.eval()\t# Coloca o modelo em modo de avaliação.\n    \n    prog_bar = tqdm(data_loader, total=len(data_loader), desc=\"[Validação]\")\n    validation_loss = 0\n    \n    # Itera sobre os lotes de dados do data_loader de validação.\n    for i, data in enumerate(prog_bar):\n        if data is None or data[0] is None:\n            continue\n\n        imgs, annotations = data\n        imgs = list(img.to(device) for img in imgs)\n        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n        \n        was_training = model.training\t# Guarda o estado atual (que é 'eval').\n        model.train()\t# Muda para modo 'train' para obter o dict de loss.\n        loss_dict = model(imgs, annotations)\n        model.train(was_training)\t\t# Restaura o estado original ('eval').\n\n        losses = sum(loss for loss in loss_dict.values())\n        \n        if torch.isfinite(losses): \n            validation_loss += losses.item()\n        \n        prog_bar.set_postfix(loss=losses.item())\n        \n    # Retorna a média da perda de validação.\n    return validation_loss / len(data_loader) if len(data_loader) > 0 else 0.0\n\nprint(\"Célula 6: Funções de Engine definidas (com 'evaluate' corrigido).\")","metadata":{"execution":{"iopub.status.busy":"2025-06-24T12:55:07.592452Z","iopub.execute_input":"2025-06-24T12:55:07.595445Z","iopub.status.idle":"2025-06-24T12:55:07.630348Z","shell.execute_reply.started":"2025-06-24T12:55:07.595233Z","shell.execute_reply":"2025-06-24T12:55:07.629002Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Célula 6: Funções de Engine definidas (com 'evaluate' corrigido).\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\n# --- Função para criar o modelo (versão específica para ResNet-50) ---\ndef get_model_fasterrcnn_resnet50(num_classes):\n    \"\"\"\n    Carrega um modelo Faster R-CNN pré-treinado (backbone ResNet-50)\n    e o adapta para o número de classes desejado.\n    \"\"\"\n    \n    # Carrega o modelo com pesos pré-treinados.\n    model = fasterrcnn_resnet50_fpn(weights='FasterRCNN_ResNet50_FPN_Weights.DEFAULT')\n    \n    # Obtém o número de features da camada de predição.\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    \n    # Substitui a camada de predição por uma nova, adequada ao nosso número de classes.\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:55:07.631678Z","iopub.execute_input":"2025-06-24T12:55:07.633150Z","iopub.status.idle":"2025-06-24T12:55:07.667391Z","shell.execute_reply.started":"2025-06-24T12:55:07.633106Z","shell.execute_reply":"2025-06-24T12:55:07.666166Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# --- Função para criar o modelo ---\ndef get_model_fasterrcnn_mobilenet(num_classes):\n    \n    \"\"\"\n    Cria um modelo Faster R-CNN usando um backbone MobileNetV2.\n    Este é um modelo mais leve que o ResNet-50.\n    \"\"\"\n    \n    # Carrega o extrator de features (backbone) do MobileNetV2 pré-treinado no ImageNet.\n    backbone = torchvision.models.mobilenet_v2(weights='MobileNet_V2_Weights.DEFAULT').features\n    \n    # O número de canais de saída do backbone é necessário para a próxima camada.\n    backbone.out_channels = 1280\n    \n    # Define o gerador de âncoras para a Region Proposal Network (RPN).\n    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),))\n    \n    # Define a camada de RoI pooling.\n    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)\n    \n    # Constrói o modelo Faster R-CNN com os componentes definidos.\n    model = FasterRCNN(backbone, num_classes=num_classes, rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:55:07.668639Z","iopub.execute_input":"2025-06-24T12:55:07.669078Z","iopub.status.idle":"2025-06-24T12:55:07.704744Z","shell.execute_reply.started":"2025-06-24T12:55:07.669046Z","shell.execute_reply":"2025-06-24T12:55:07.702930Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\n# --- Função para criar o modelo ---\ndef get_model_fasterrcnn_efficientnet(num_classes):\n    \"\"\"\n    Cria um modelo Faster R-CNN usando um backbone EfficientNet-B0.\n    \"\"\"\n    \n    # Carrega o extrator de features (backbone) do EfficientNet-B0 pré-treinado.\n    backbone = torchvision.models.efficientnet_b0(weights='EfficientNet_B0_Weights.DEFAULT').features\n    \n    # Define manualmente o número de canais de saída do backbone.\n    backbone.out_channels = 1280\n    \n    # Define o gerador de âncoras e o RoI Pooler, similar ao MobileNet.\n    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),))\n    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)\n    \n    # Constrói o modelo Faster R-CNN final.\n    model = FasterRCNN(backbone, num_classes=num_classes, rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler)\n    \n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:55:07.705990Z","iopub.execute_input":"2025-06-24T12:55:07.706297Z","iopub.status.idle":"2025-06-24T12:55:07.737108Z","shell.execute_reply.started":"2025-06-24T12:55:07.706273Z","shell.execute_reply":"2025-06-24T12:55:07.734266Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# =========================\n# Célula 11: Validação em Novo Agrupamento COCO (validação no grupo de validação do COCO 2014)\n# Este código realiza a validação dos modelos treinados usando um subconjunto (801 imagens) do grupo de validação do COCO 2014 (val2014).\n# =========================\n\nNOVO_DATA_DIR = '/kaggle/input/val2014/val2014'\nNOVO_COCO = '/kaggle/input/coco-image-caption/annotations_trainval2014/annotations/instances_val2014.json'\n\n# Cria o novo dataset e dataloader para validação (usando 801 imagens)\nnovo_dataset = FilteredCOCODataset(\n    root=NOVO_DATA_DIR,\n    annotation=NOVO_COCO,\n    transforms=get_transform(train=False),\n    cats=config.CATEGORIAS_DESEJADAS,\n    limit=500\n)\nnovo_data_loader = DataLoader(\n    novo_dataset,\n    batch_size=1,\n    shuffle=False,\n    num_workers=config.NUM_WORKERS_DL,\n    collate_fn=collate_fn\n)\nprint(f\"Novo agrupamento: {len(novo_dataset)} imagens.\\n\")\n\n# Lista de modelos para validar (cada dicionário contém o nome, caminho do modelo e função de criação)\nmodelos_info = [\n    {\n        \"nome\": \"ResNet50\",\n        \"caminho\": \"/kaggle/input/fastercnn_resnet50/tensorflow2/default/1/best_model_checkpoint.pth\",\n        \"funcao\": get_model_fasterrcnn_resnet50  # ou get_model_fasterrcnn_resnet50 se for esse o nome\n    },\n    {\n        \"nome\": \"EfficientNetB0\",\n        \"caminho\": \"/kaggle/input/fastercnn_efficientnetb0/tensorflow2/default/1/best_model_checkpoint.pth\",\n        \"funcao\": get_model_fasterrcnn_efficientnet\n    },\n    {\n        \"nome\": \"MobileNetV2\",\n        \"caminho\": \"/kaggle/input/fastercnn_mobilenetv2/tensorflow2/default/1/best_model.pth\",\n        \"funcao\": get_model_fasterrcnn_mobilenet\n    }\n]\n\n# Loop para validar cada modelo no grupo de validação do COCO 2014\nfor info in modelos_info:\n    print(f\"=== Validação para {info['nome']} ===\")\n    best_model_path = info[\"caminho\"]\n    modelo = info[\"funcao\"](config.NUM_CLASSES)\n    checkpoint = torch.load(best_model_path, map_location=config.DEVICE)\n    modelo.to(config.DEVICE)\n\n    # Avaliação do loss\n    loss_novo_agrupamento = evaluate(modelo, novo_data_loader, config.DEVICE)\n    print(f\"Loss no novo agrupamento: {loss_novo_agrupamento:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T12:55:07.738903Z","iopub.execute_input":"2025-06-24T12:55:07.739842Z","iopub.status.idle":"2025-06-24T14:20:55.210631Z","shell.execute_reply.started":"2025-06-24T12:55:07.739779Z","shell.execute_reply":"2025-06-24T14:20:55.208134Z"}},"outputs":[{"name":"stdout","text":"loading annotations into memory...\nDone (t=12.77s)\ncreating index...\nindex created!\nNovo agrupamento: 497 imagens.\n\n=== Validação para ResNet50 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Validação]:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcb17b29ec644afaafb1059d51d0fc37"}},"metadata":{}},{"name":"stdout","text":"Loss no novo agrupamento: 2.4393\n=== Validação para EfficientNetB0 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Validação]:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"639931d59fa4400f84ee467e29973146"}},"metadata":{}},{"name":"stdout","text":"Loss no novo agrupamento: 2.3135\n=== Validação para MobileNetV2 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Validação]:   0%|          | 0/497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f08985fa805e412fa943491dc4d66aeb"}},"metadata":{}},{"name":"stdout","text":"Loss no novo agrupamento: 2.2242\n","output_type":"stream"}],"execution_count":9}]}